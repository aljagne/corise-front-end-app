{"text": " Welcome to data skeptic machine intelligence, our podcast series exploring contemporary topics in artificial general intelligence and large language models. Do all of you know about the grandmother neuron paradox? I'm not sure if it's called exactly that, but the idea that, well, the fallacy that maybe a surgeon could open up your brain, take a real precision tweezers, grab a single neuron and yank it out of your brain. And as long as that surgeon picked exactly the right neuron, you would completely forget your grandmother. Which one? I'm not sure. You ought to have two, right? Maybe more. But this nonsensical idea demonstrates for us that the brain is a little bit more holographic. Information is distributed across it. And I think it's worth anthropomorphizing the grandmother neuron idea to large language models. Take any deep learning model in fact, and remove one single parameter. I mean, heck, this is what dropout does. That neural network should be virtually unchanged by one small perturbation in its design. So surely they can stand to lose a few parameters. How many exactly? I guess that depends on what sort of trade-off you're willing to have. At present, my assessment is that this is a true practitioner's art, at least in the state of the art as it is in 2023. There's a lot of discussion about LoRa, low rank approximation, but even that feels pretty low level requiring a lot of wizardry on a part of the person deploying it. That's why I was totally intrigued when I came across the paper we're going to discuss in today's interview. That paper is Cuttlefish, low rank model training without all the tuning. You know, sometimes when I look at deep learning models and I consider all the tuning, it looks like a Moog synthesizer with dozens of knobs. Maybe Wendy Carlos knows what to do with them. But I have no idea how to fine tune all those oscillators and whatnot. Could I get just one knob? Maybe that's what Cuttlefish can give me. Let's find out. My name is Hongyi Wang. I'm currently with the machine learning department at Carnegie Mellon University. I'm a senior researcher there. I previously got my PhD degree from the computer science department at the University of Wisconsin-Madison. So my research is mainly about the intersection of system and machine learning. Now it's called MLC, fancy name. They're kind of like optimize the system to run your machine learning code or model or algorithm better on your hardware. So basically that's what I'm doing. Can you tell me a little bit about the focus of your research? I mostly study distributed machine learning. So basically parallel computing. Currently as the model becomes bigger and data also becomes bigger, so one GPU seems not enough to train our model or even do the so-called fine tuning these days. So my research is about how you can use more, utilize more CPU cores or more GPUs to accelerate your computing during training or model fine tuning. But there the problem we care about is like when you use 1000 GPUs together, how can you get 1000 times faster? So that's what we call scaling problem. Now we are kind of doing kind of well for the smaller model. Let's say if you want to train a residual network or convolution network, it seems to be fine. We solve the scaling problem okay. But for large language models, sometimes we need to go beyond what we call data parallelism. That makes the problem harder. So we are working hard on that recently. So I have written a fair amount of Java code in my professional career and on one or two occasions had a collaborator who knew about the underlying architecture of the JVM much better than I did and was able to go do some very fine tuned optimizations about the way garbage collection work to make everything just function a lot better. I don't have that skill set and I kind of don't want it. I'd rather just sort of do what I know there and let someone else handle that. But then I try and carry that analogy over to deep learning where I am aware of things like batch normalization and the vanishing gradient and concepts like this or even the transformer architecture and how it works. Do you think it's necessary that people who use deep learning know the fundamentals and the methods in that way or are we maybe entering a world where someone could just engineer on top of that technology? Right. I think that's pretty kind of like a visionary problem and we're actually working very hard on that. So as I just mentioned, if I do not have a system like knowledge, I just work on fundamental science, let's say, and I want to use the foundation model or kind of like deep learning to kind of like power my research. It's basically kind of like impossible in the past. But now we can kind of like have PyTorch, like TensorFlow, it seems to become easier, but still very hard. Let's say if I were even given, let's say, like 1000 GPU from Microsoft, I don't even know how to use the 1000 GPU together. So that's the problem. We are kind of like solving, kind of like come up with another layer in between from application people and engineer and also the hardware. So that's what we are doing. Hopefully we can automatically deploy your code on the hardware in the optimized way or in kind of like an optimal way. Yeah, I know it won't be the focus of our main discussion today for the paper, Cuttlefish, that I invite you to talk about. But before we get into that, could you just reference a little bit about the hardware? Is that like FPGAs or what are you thinking there? Actually, yeah, I know the world is currently GPU dominated or TPU dominated, but we're actually thinking about a wider picture. So there are a whole bunch of like used hardware everywhere, right? Even in the AWS cloud, there are data centers, there are a lot of like used GPUs, also the community hardware. So one of my research goals is to try to combine everything together, kind of like reuse the machine, everything, your CPU cores. Then there, like if you kind of calculate the theoretical flaws, that's actually comparable if we use as much, you know, those used hardware as possible. It's comparable to the A100. But the problem is there is how you can use them efficiently together. So that's what some of my past work and my future work will try to tackle. Well, you know, theoretically you can solve this, right? You can build a very, very fancy cost model and you solve how you can parallelize your computation across all the hardware. It's basically a graph partitioning problem, but theoretically we can solve that. It's kind of like the engineering problem. How can we like scheduling things well? How can we use them cleverly? And how can we even handle the fault tolerance? So like when we have a whole bunch of hardware together, it's very likely like some of them will fail and it seems like that. Yeah, that's definitely possible. And I'm devoting my time to work on that. Well, if we look back in recent history, it's not like NLP has changed overnight. I mean, maybe it did in some ways when some of these amazing models came out. But we can look back to like Word2vec as a pretty impressive technology that's, I guess, a decade old. At what point or was there a point when you first took notice and said, wow, something's happening here? Mostly, when I joined this community, it's mostly there's no NLP scaling problem yet, I would say. So at that time, people are excited about like VGG, how you can scale VGG on ImageNet training. So that already becomes a problem because the ImageNet is huge. And also, there's a whole bunch of images there. There's currently one, if I remember correctly, it's kind of like one million images inside of the data set. So even kind of like one epoch, now it's called epoch of training, it takes a whole bunch of time. So at that time, we were trying to optimize things like how can we make the model go pass through the data passes as many as possible and as fast as possible. So I would say, you know, like after ImageNet challenging the benchmark, things start to become different. Like all of the people are starting to care about how to make things faster and how can we optimize all the software stack to try to keep the top of the benchmark as much as possible. And now we have some other benchmark, you know, kind of like people are hitting the let's say VKUNA leaderboard or Puggingface leaderboard. It seems to be the newer version of ImageNet in this current era. So I've heard some reports about the cost of running some of these large language models. Like maybe it costs one penny for a thousand requests of chat GPT or something like that. And it sounds so low when you describe it as a commodity, but actually in some sense, maybe it's expensive because we're doing things inefficiently or something along those lines. Yeah. Where are the opportunities to make efficiency gains here? Right. So kind of in terms of the data center computing, I think it's if we can, everything comes through the most advanced GPU. I think that has been optimized quite well, actually. So in the sense that people may not even want to use, there's a whole bunch of different parallelism, right? So we have data parallelism model and also the pipeline parallelism. So different of them thing. And the people have realized that when you can use them together, you can reach to some sort of like optimal performance. But now in the data center setting, I don't think people want to even use that. They just use the 4D shard data parallel released by it's also called Deep Speed Zero Stage 3, but whatever. They're just using that. It's because the hardware is highly optimized in the data center. The kind of cross node bandwidth is high enough. So it's fine. So everything is optimized quite well. But when we consider if we want to scale things out of the data center, right? We want to combine all the real hardware or the community hardware. So that's different. So the communication can be really the bottleneck there. So we consider there are two clusters located on different, one on the East Coast, one on the West Coast. So even one communication can take quite a long time. So how can we do that? How can we reduce the communication among there and try to do less communication as much as possible, focus more on local computing? Yeah, that's I think where the opportunity is. Well, there's a popular technique I've heard called LoRa, the low rank approximation, where I guess my, you can correct me if I'm wrong, but my understanding of it is take the big model that's been trained in some expensive and maybe inefficient way and then find a way to compress it or reduce it or something like that. Yeah. Yeah. Is that fair? And is that a good strategy? Uh, right. I think it's really, it really depends. So my personal experience is that if we are really GPU or computing power constraint, say if we can't even train the 7 billion model, so LoRa is a perfect technology to democratize the model to a whole bunch of people. Right. So currently I think LoRa, the LoRa like software stack and also it's a variant. Let's say the QLoRa is optimized really well to basically just run our MacBook. So that's, that's perfect. I think a lot of like amazing application has been built on top of that to generate their own images, things like that, stable diffusion or some more serious, let's say accuracy driven application. Let's say I myself is building a medical chat board right now and I'm just doing some like a pre-training plus fine tuning on some medical publications. Let's say there's a kind of like something called PubMed. There, I think if we have enough or kind of like decent amount of computing resources, LoRa seems to lead to some accuracy drops there. So let's say if we, if you tune a 65 billion model using LoRa, it won't perform as good as we find for fine tune a 30 billion model. So I think it's really depends on applications, but I personally think LoRa is a very amazing work for democratizing the powerfulness of the large language model to a wide amount of users. Well, how would you contrast LoRa to Cuttlefish and maybe also can you at the same time introduce Cuttlefish? Oh yeah, definitely. So Cuttlefish is actually not even the first attempt for this line of work. Discuss with the LoRa team, we had had a few official meetings actually. So Cuttlefish is something like, so we're just treating the model, it's a model is just too huge. So as a beginning, we are thinking about how can we make kind of like mixing a little bit easier. Turns out we can just treat a low rank model. So low rank is basically, it's a kind of like a basic linear algebra. So our model is basically a whole bunch of, we can understand that the whole bunch of matrix concatenating together and we are kind of like computing one matrix multiplication by one matrix multiplication. So low rank approximation is basically we approximate the matrix multiplication using fewer number of flops. And also we can also make the matrix a little bit smaller, such that the computation can be a bit faster. We also save a little bit of GPU memory and also communication become faster because we are essentially training a smaller model. That's basically the basic technology behind Cuttlefish. And actually we, at the beginning, we published a paper in 2021 called Pufferfish. So that's the very initial work. It's just doing this approximation and the training from scratch. And turns out when you train the low rank model, it doesn't work as well as a full rank model because we kind of like to reduce a lot of model capacity. It doesn't generalize well. Then we kind of like introduced several technologies there. Some of the layer it's hard to approximate, right? If you kind of like approximate that it will cause a huge amount of accuracy drop. For some of the layer it's fine. So that's the first technology. We're just selectively factorizing some of the layers in the neural network. Second technology is that you may not want to go low rank in the very beginning. So it's mostly like the sparse training, right? So in the beginning you want to do this and you gradually make your model become sparse. In the full rank to low rank training, we also doing things like that. In the beginning, we are doing full rank model and then we gradually start to convert from full rank to low rank. So that's called Pufferfish published in 2021. Pufferfish took that a step further. It's basically so when we switch from full rank to low rank and which layers we don't want to factorize. So those are a lot of like heavy hyperparameter tuning. And for each of the layer, how much should we probe? Right. That's also a kind of like a fundamental problem faced in the sparse training community. For each of the layer, you may not want to sparsify them in the same ratio. Pufferfish take everything automatic and we kind of like make the training. During the training, we detect some of the heuristics and we try to make everything automatic. So basically select each of the layers. Select only the layer that's tolerable for low rank factorization and automatically detect when should we switch from full rank to low rank. Basically make everything more hyperparameter free. It's not totally hyperparameter free, but something like more automatic. Well, if we're to maybe frame it in terms of tradeoffs, if I want to take advantage of these efficiencies, do I have to give something up? And if so, what? Most of the critical part is that we have to give up some of the accuracy if we don't do it quite well. The entire work is basically depend on how much the redundancy is in the model. So if the model parameter does not contain redundancy at all, this method will fail. So I have to be very honest. It's about how to detect the model's parameters redundancy. If there's no redundancy, don't use this. And if there's enough redundancy, then we should detect what's the level of the redundancy and we should remove the exact amount of the redundancy. So in the PowerFish paper, we did that in a very heuristic way. Sometimes it won't work. You can like, as you mentioned, we will sacrifice some accuracy. In Cuttlefish, we are trying to optimize that really well to automatically detect the redundancy level and then we just remove those appropriate amount of redundancy. And it turns out it kind of like mitigates accuracy drop very well. I appreciate your point. If there's no redundancy, there's nothing to, there's no advantage to be gained. Yeah. But when I hear about these models that have, you know, increasingly billions of parameters, it's hard to believe there isn't redundancy here. Is that a fair intuition? I think so. So I think the Scouting Law paper is a very, very good one. I actually encourage people to read. So basically that's basically telling us that the data should scale proportionately to the model size, right? So otherwise, if we just scale the model up, let's say to one training or whatever, it will only increase the redundancy in the model parameters. That will make PowerFish work better or Cuttlefish work better, but that's not what we want to see actually in the application. Like we should really control that very well. I guess that's basically my intuition. There's no kind of like a qualitative way to do that yet. I already encourage people to look into that research direction. Well, when I think about the people training large language models, in my head, there's two categories. There's people like the ones at OpenAI who are working on the foundational models. Yeah. Maybe training the next DaVinci or ChatGPT or whatever it is. And then there are those who want to extend it to their own domain. Right. Who should, which or both, who should be using Cuttlefish? Well, I really want to say both, but for the first principle, Cuttlefish is designed for pre-training, I would say. For fine tuning, I think Laura works reasonably well, but it doesn't mean like the Cuttlefish cannot support fine tuning. It's mostly like you take the model checkpoint, you just decompose them and you just keep fine-tune on your own data set. Yeah. I would say mostly for pre-training, at least in the current stage. And can you talk a little bit about some empirical results? How do you measure the improvement gains you get with the methods? First of all, it's a very, the most straightforward one is the model size. Right? So like when we kind of decompose the model. So currently let's say, actually right now I'm training a 1 billion llama to model like using Cuttlefish. We can kind of like reach to, we just train something like a 60% smaller, to some model with a 60%, you can roughly understand that's a 600 million, let's say llama to model subset of the pile data set. It's already reached to the same perplexity in the pre-training stage. So, you know, that's the first, the rationale there. And also like, since we don't have to load everything in the GPU, right? So there are kind of like a second again, there is basically the GPU memory. And also most important thing is basically we compute things faster. So currently we can reach to 1.4 times like faster end to end training speed compared to the full rank or kind of like a dense llama model. So those are the, the gain. The loose areas, like we kind of like a drop a little bit of the perplexity. So roughly speaking the same, but the still a little bit of drop. I'm still working on that tweaking different, I guess, hyperparameters and then try to, try to make things better. And in a lot of more general cases, I would have to do something with my own hyperparameters, maybe a grid search or I don't know, some clever way of picking them. Can I now step away from that and let Cuttlefish kind of auto optimize those sorts of things for me? So the best we can do is say, if we have a concrete set of hyperparameters used for training the dense model, let's say llama, I think that's kind of like there's a standard like a set of hyperparameters, like what's the learning rate that you need to use? What's that batch size? I think it's around two million things like that. So you have a kind of like a standard set of hyperparameter to get started with. Cuttlefish is doing something to guarantee you that if you still use the same set of hyperparameters, you will reach to a good accuracy. That's what we can guarantee. But there's a more fundamental question that when you train low-rank model, is there, you know, does there exist a better set of hyperparameter that can make the high, you know, like a low-rank model to better the full-rank model? I think there is, but like from the design principle, we don't want to, you know, like dump that effort into users. We just want to make the process as easy as possible. You can just take the original set of hyperparameter, take our technology and just train everything will be handled. There's no extra hyperparameter to tune. Everything will converge almost as same as the original model. Well, I suppose everyone's data set is going to be a little bit different and unique, but at least in the paper you report it's 1.2 times faster to train and 5.6 times smaller than a full-ranked model. Is there any reason with those good stats in mind not to just adopt these processes? So I think that's probably my fault. Like, so we still need a very good software to basically support the things like that. So, because there's a whole bunch of amazing algorithms. I think one reason people are not using that is basically the software engineering has not been done right. So that's something I'm always trying to do is basically I want to write a model compiler, basically taking whatever kind of model and then, you know, the model compiler will basically just go through all the layers and try to detect redundancy there. And then if the redundancy hit some of the criteria, it will basically just do the transfer that from full-rank to low-rank automatically. I think that support is not there yet. So that's I should work on that. And I'm actually trying to work on that right now. So that's the first thing. The second thing is basically, as you mentioned, for different applications, I think there are still some of the hyperparameter has to be tuned appropriately. Otherwise, I'm not sure if the low-rank method will work as good as, let's say, the vision and the language models. Those are the two mainstream models. But I think a lot of people care about, let's say, graph data and or even like tabular data. So those are things like I haven't explored very well yet. Yeah. But I think the most most important part is the software engineering that has been done appropriately and very efficiently, such that people would like to really adopt. Well, when I think historically, if someone said, I'm interested in machine learning, I would encourage them to get, you know, probably an advanced degree, focus in math and stats and algorithms and things along these lines. It seems like maybe there are more lines of opportunity in machine learning now. For example, like people interested in software are not always interested in compilers, but some people are compiler people. Do you have any advice for people who want to take a sort of really engineering MLOps kind of approach? What are good inroads to be learning this area? I would really recommend people to just start to play with. Let's say just download the Lama2, you know, just the checkpoint and just start to, let's say, put that into or just fine tune on their own data. I just use a hugging face. I think hugging face support is kind of like a provide very good software support for everything. And just start to try to play and process the data, the tokenized things, like what does that even mean? And just the factor, I think, you know, just start to do the fine tuning. And then when they are working on that, then we will start to see, you know, what's actually the task is, right? So it's just a basic next-door prediction. And then they will kind of like try to realize which part doesn't go right. And what does the loss function mean? What does gradient mean? Yeah, I think everything will naturally come into play. Good advice. Yeah. I learned a lot by getting my hands dirty. I didn't learn everything in a classroom, for sure. Same thing here. Well, yeah, classroom is also good. And yeah, I guess people still, if you can go to college or a graduate school, I think the principle learning, machine learning in the principle of the way is still important to understand, you know, what's actually going on, you know, what does generalization mean, things like that. But currently, it seems like the whole field is going into the, I think, toward application engineering heavy style. So that's why I think practice would be really be important. And people, as you mentioned, I really agree with you, Kyle, like people should get their hands dirty and try to really experience things. What's coming next for you in your research? My vision is mostly currently like to into kind of two separated fields. As for people in academia, we shouldn't really compete with, try to compete with OpenAI. I think that's a little bit impossible to achieve AGI there. What we should really do in the MLCs community is basically I'm interested in helping, let's say, people do fundamental science. There's a whole area called AI for Science. I see big opportunities there that people can leverage the power of foundation model to really help their applications. Let's say gene protein structure prediction, like a drug prediction, and also the kind of like a medical diagnosis assistant I just mentioned to you. So that's the one thing I'm trying to do right now. So how can we leverage the powerfulness of the foundation model to build the domain specific applications and really to help like mostly scientists in my case? And another is basically how to democratize, really democratize the foundation model to wider, you know, amount of people. So like when people do not have like advanced computing resources, what can we do for them? So the software can be optimized really well to make people also use foundation model really well. It turns out actually we can also like group the commodity hardware together to even train the powerful foundation model. So that's what I'm excited about for the future research. And is there anywhere listeners can follow you online if they want to keep up with that as well? Yeah, definitely. So one thing is definitely Twitter. I think I already shared Twitter, my Twitter account. And I think another is basically my Google Scholar page and my personal home page. Yeah, I can probably send it to you after this, after the recording. That'd be good. Yeah. We'll have links to all the above in the show notes for people to follow up with. Well, thank you so much for taking the time to come on and share your work. Oh, yeah. Thanks a lot for having me, Kyle. That's this is really, really exciting.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.28, "text": " Welcome to data skeptic machine intelligence, our podcast series exploring contemporary", "tokens": [50364, 4027, 281, 1412, 19128, 299, 3479, 7599, 11, 527, 7367, 2638, 12736, 14878, 50778], "temperature": 0.0, "avg_logprob": -0.2082026077039314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.03896750137209892}, {"id": 1, "seek": 0, "start": 8.28, "end": 15.32, "text": " topics in artificial general intelligence and large language models.", "tokens": [50778, 8378, 294, 11677, 2674, 7599, 293, 2416, 2856, 5245, 13, 51130], "temperature": 0.0, "avg_logprob": -0.2082026077039314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.03896750137209892}, {"id": 2, "seek": 0, "start": 15.32, "end": 19.44, "text": " Do all of you know about the grandmother neuron paradox?", "tokens": [51130, 1144, 439, 295, 291, 458, 466, 264, 14317, 34090, 26221, 30, 51336], "temperature": 0.0, "avg_logprob": -0.2082026077039314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.03896750137209892}, {"id": 3, "seek": 0, "start": 19.44, "end": 24.8, "text": " I'm not sure if it's called exactly that, but the idea that, well, the fallacy that", "tokens": [51336, 286, 478, 406, 988, 498, 309, 311, 1219, 2293, 300, 11, 457, 264, 1558, 300, 11, 731, 11, 264, 2100, 2551, 300, 51604], "temperature": 0.0, "avg_logprob": -0.2082026077039314, "compression_ratio": 1.4924623115577889, "no_speech_prob": 0.03896750137209892}, {"id": 4, "seek": 2480, "start": 24.8, "end": 30.560000000000002, "text": " maybe a surgeon could open up your brain, take a real precision tweezers, grab a single", "tokens": [50364, 1310, 257, 22913, 727, 1269, 493, 428, 3567, 11, 747, 257, 957, 18356, 683, 22544, 433, 11, 4444, 257, 2167, 50652], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 5, "seek": 2480, "start": 30.560000000000002, "end": 34.16, "text": " neuron and yank it out of your brain.", "tokens": [50652, 34090, 293, 288, 657, 309, 484, 295, 428, 3567, 13, 50832], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 6, "seek": 2480, "start": 34.16, "end": 38.72, "text": " And as long as that surgeon picked exactly the right neuron, you would completely forget", "tokens": [50832, 400, 382, 938, 382, 300, 22913, 6183, 2293, 264, 558, 34090, 11, 291, 576, 2584, 2870, 51060], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 7, "seek": 2480, "start": 38.72, "end": 40.36, "text": " your grandmother.", "tokens": [51060, 428, 14317, 13, 51142], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 8, "seek": 2480, "start": 40.36, "end": 41.36, "text": " Which one?", "tokens": [51142, 3013, 472, 30, 51192], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 9, "seek": 2480, "start": 41.36, "end": 42.36, "text": " I'm not sure.", "tokens": [51192, 286, 478, 406, 988, 13, 51242], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 10, "seek": 2480, "start": 42.36, "end": 43.36, "text": " You ought to have two, right?", "tokens": [51242, 509, 13416, 281, 362, 732, 11, 558, 30, 51292], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 11, "seek": 2480, "start": 43.36, "end": 44.36, "text": " Maybe more.", "tokens": [51292, 2704, 544, 13, 51342], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 12, "seek": 2480, "start": 44.36, "end": 50.760000000000005, "text": " But this nonsensical idea demonstrates for us that the brain is a little bit more holographic.", "tokens": [51342, 583, 341, 297, 892, 694, 804, 1558, 31034, 337, 505, 300, 264, 3567, 307, 257, 707, 857, 544, 38541, 2662, 299, 13, 51662], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 13, "seek": 2480, "start": 50.760000000000005, "end": 53.44, "text": " Information is distributed across it.", "tokens": [51662, 15357, 307, 12631, 2108, 309, 13, 51796], "temperature": 0.0, "avg_logprob": -0.12054630861443988, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.10957514494657516}, {"id": 14, "seek": 5344, "start": 53.44, "end": 58.72, "text": " And I think it's worth anthropomorphizing the grandmother neuron idea to large language", "tokens": [50364, 400, 286, 519, 309, 311, 3163, 22727, 32702, 3319, 264, 14317, 34090, 1558, 281, 2416, 2856, 50628], "temperature": 0.0, "avg_logprob": -0.15033187447013435, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.004754138644784689}, {"id": 15, "seek": 5344, "start": 58.72, "end": 59.92, "text": " models.", "tokens": [50628, 5245, 13, 50688], "temperature": 0.0, "avg_logprob": -0.15033187447013435, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.004754138644784689}, {"id": 16, "seek": 5344, "start": 59.92, "end": 65.03999999999999, "text": " Take any deep learning model in fact, and remove one single parameter.", "tokens": [50688, 3664, 604, 2452, 2539, 2316, 294, 1186, 11, 293, 4159, 472, 2167, 13075, 13, 50944], "temperature": 0.0, "avg_logprob": -0.15033187447013435, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.004754138644784689}, {"id": 17, "seek": 5344, "start": 65.03999999999999, "end": 67.98, "text": " I mean, heck, this is what dropout does.", "tokens": [50944, 286, 914, 11, 12872, 11, 341, 307, 437, 3270, 346, 775, 13, 51091], "temperature": 0.0, "avg_logprob": -0.15033187447013435, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.004754138644784689}, {"id": 18, "seek": 5344, "start": 67.98, "end": 74.47999999999999, "text": " That neural network should be virtually unchanged by one small perturbation in its design.", "tokens": [51091, 663, 18161, 3209, 820, 312, 14103, 44553, 538, 472, 1359, 40468, 399, 294, 1080, 1715, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15033187447013435, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.004754138644784689}, {"id": 19, "seek": 5344, "start": 74.47999999999999, "end": 78.02, "text": " So surely they can stand to lose a few parameters.", "tokens": [51416, 407, 11468, 436, 393, 1463, 281, 3624, 257, 1326, 9834, 13, 51593], "temperature": 0.0, "avg_logprob": -0.15033187447013435, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.004754138644784689}, {"id": 20, "seek": 5344, "start": 78.02, "end": 79.02, "text": " How many exactly?", "tokens": [51593, 1012, 867, 2293, 30, 51643], "temperature": 0.0, "avg_logprob": -0.15033187447013435, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.004754138644784689}, {"id": 21, "seek": 7902, "start": 79.5, "end": 83.78, "text": " I guess that depends on what sort of trade-off you're willing to have.", "tokens": [50388, 286, 2041, 300, 5946, 322, 437, 1333, 295, 4923, 12, 4506, 291, 434, 4950, 281, 362, 13, 50602], "temperature": 0.0, "avg_logprob": -0.14072670424280087, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.5769302248954773}, {"id": 22, "seek": 7902, "start": 83.78, "end": 89.38, "text": " At present, my assessment is that this is a true practitioner's art, at least in the", "tokens": [50602, 1711, 1974, 11, 452, 9687, 307, 300, 341, 307, 257, 2074, 32125, 311, 1523, 11, 412, 1935, 294, 264, 50882], "temperature": 0.0, "avg_logprob": -0.14072670424280087, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.5769302248954773}, {"id": 23, "seek": 7902, "start": 89.38, "end": 92.82, "text": " state of the art as it is in 2023.", "tokens": [50882, 1785, 295, 264, 1523, 382, 309, 307, 294, 44377, 13, 51054], "temperature": 0.0, "avg_logprob": -0.14072670424280087, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.5769302248954773}, {"id": 24, "seek": 7902, "start": 92.82, "end": 98.0, "text": " There's a lot of discussion about LoRa, low rank approximation, but even that feels pretty", "tokens": [51054, 821, 311, 257, 688, 295, 5017, 466, 6130, 41873, 11, 2295, 6181, 28023, 11, 457, 754, 300, 3417, 1238, 51313], "temperature": 0.0, "avg_logprob": -0.14072670424280087, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.5769302248954773}, {"id": 25, "seek": 7902, "start": 98.0, "end": 102.69999999999999, "text": " low level requiring a lot of wizardry on a part of the person deploying it.", "tokens": [51313, 2295, 1496, 24165, 257, 688, 295, 25807, 627, 322, 257, 644, 295, 264, 954, 34198, 309, 13, 51548], "temperature": 0.0, "avg_logprob": -0.14072670424280087, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.5769302248954773}, {"id": 26, "seek": 7902, "start": 102.69999999999999, "end": 106.5, "text": " That's why I was totally intrigued when I came across the paper we're going to discuss", "tokens": [51548, 663, 311, 983, 286, 390, 3879, 35140, 562, 286, 1361, 2108, 264, 3035, 321, 434, 516, 281, 2248, 51738], "temperature": 0.0, "avg_logprob": -0.14072670424280087, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.5769302248954773}, {"id": 27, "seek": 7902, "start": 106.5, "end": 108.1, "text": " in today's interview.", "tokens": [51738, 294, 965, 311, 4049, 13, 51818], "temperature": 0.0, "avg_logprob": -0.14072670424280087, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.5769302248954773}, {"id": 28, "seek": 10810, "start": 108.1, "end": 114.02, "text": " That paper is Cuttlefish, low rank model training without all the tuning.", "tokens": [50364, 663, 3035, 307, 9431, 10972, 11608, 11, 2295, 6181, 2316, 3097, 1553, 439, 264, 15164, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1549614521495083, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.31368255615234375}, {"id": 29, "seek": 10810, "start": 114.02, "end": 118.53999999999999, "text": " You know, sometimes when I look at deep learning models and I consider all the tuning, it looks", "tokens": [50660, 509, 458, 11, 2171, 562, 286, 574, 412, 2452, 2539, 5245, 293, 286, 1949, 439, 264, 15164, 11, 309, 1542, 50886], "temperature": 0.0, "avg_logprob": -0.1549614521495083, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.31368255615234375}, {"id": 30, "seek": 10810, "start": 118.53999999999999, "end": 121.89999999999999, "text": " like a Moog synthesizer with dozens of knobs.", "tokens": [50886, 411, 257, 3335, 664, 26617, 6545, 365, 18431, 295, 46999, 13, 51054], "temperature": 0.0, "avg_logprob": -0.1549614521495083, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.31368255615234375}, {"id": 31, "seek": 10810, "start": 121.89999999999999, "end": 124.63999999999999, "text": " Maybe Wendy Carlos knows what to do with them.", "tokens": [51054, 2704, 21850, 19646, 3255, 437, 281, 360, 365, 552, 13, 51191], "temperature": 0.0, "avg_logprob": -0.1549614521495083, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.31368255615234375}, {"id": 32, "seek": 10810, "start": 124.63999999999999, "end": 128.64, "text": " But I have no idea how to fine tune all those oscillators and whatnot.", "tokens": [51191, 583, 286, 362, 572, 1558, 577, 281, 2489, 10864, 439, 729, 18225, 3391, 293, 25882, 13, 51391], "temperature": 0.0, "avg_logprob": -0.1549614521495083, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.31368255615234375}, {"id": 33, "seek": 10810, "start": 128.64, "end": 130.7, "text": " Could I get just one knob?", "tokens": [51391, 7497, 286, 483, 445, 472, 26759, 30, 51494], "temperature": 0.0, "avg_logprob": -0.1549614521495083, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.31368255615234375}, {"id": 34, "seek": 10810, "start": 130.7, "end": 133.14, "text": " Maybe that's what Cuttlefish can give me.", "tokens": [51494, 2704, 300, 311, 437, 9431, 10972, 11608, 393, 976, 385, 13, 51616], "temperature": 0.0, "avg_logprob": -0.1549614521495083, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.31368255615234375}, {"id": 35, "seek": 10810, "start": 133.14, "end": 134.14, "text": " Let's find out.", "tokens": [51616, 961, 311, 915, 484, 13, 51666], "temperature": 0.0, "avg_logprob": -0.1549614521495083, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.31368255615234375}, {"id": 36, "seek": 13414, "start": 135.14, "end": 136.98, "text": " My name is Hongyi Wang.", "tokens": [50414, 1222, 1315, 307, 8868, 8461, 14499, 13, 50506], "temperature": 0.0, "avg_logprob": -0.2522937205799839, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.050286274403333664}, {"id": 37, "seek": 13414, "start": 136.98, "end": 140.73999999999998, "text": " I'm currently with the machine learning department at Carnegie Mellon University.", "tokens": [50506, 286, 478, 4362, 365, 264, 3479, 2539, 5882, 412, 47301, 376, 898, 266, 3535, 13, 50694], "temperature": 0.0, "avg_logprob": -0.2522937205799839, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.050286274403333664}, {"id": 38, "seek": 13414, "start": 140.73999999999998, "end": 142.7, "text": " I'm a senior researcher there.", "tokens": [50694, 286, 478, 257, 7965, 21751, 456, 13, 50792], "temperature": 0.0, "avg_logprob": -0.2522937205799839, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.050286274403333664}, {"id": 39, "seek": 13414, "start": 142.7, "end": 148.6, "text": " I previously got my PhD degree from the computer science department at the University of Wisconsin-Madison.", "tokens": [50792, 286, 8046, 658, 452, 14476, 4314, 490, 264, 3820, 3497, 5882, 412, 264, 3535, 295, 17977, 12, 33432, 2770, 13, 51087], "temperature": 0.0, "avg_logprob": -0.2522937205799839, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.050286274403333664}, {"id": 40, "seek": 13414, "start": 148.6, "end": 152.66, "text": " So my research is mainly about the intersection of system and machine learning.", "tokens": [51087, 407, 452, 2132, 307, 8704, 466, 264, 15236, 295, 1185, 293, 3479, 2539, 13, 51290], "temperature": 0.0, "avg_logprob": -0.2522937205799839, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.050286274403333664}, {"id": 41, "seek": 13414, "start": 152.66, "end": 155.17999999999998, "text": " Now it's called MLC, fancy name.", "tokens": [51290, 823, 309, 311, 1219, 376, 14766, 11, 10247, 1315, 13, 51416], "temperature": 0.0, "avg_logprob": -0.2522937205799839, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.050286274403333664}, {"id": 42, "seek": 13414, "start": 155.17999999999998, "end": 159.22, "text": " They're kind of like optimize the system to run your machine learning code or model or", "tokens": [51416, 814, 434, 733, 295, 411, 19719, 264, 1185, 281, 1190, 428, 3479, 2539, 3089, 420, 2316, 420, 51618], "temperature": 0.0, "avg_logprob": -0.2522937205799839, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.050286274403333664}, {"id": 43, "seek": 13414, "start": 159.22, "end": 161.42, "text": " algorithm better on your hardware.", "tokens": [51618, 9284, 1101, 322, 428, 8837, 13, 51728], "temperature": 0.0, "avg_logprob": -0.2522937205799839, "compression_ratio": 1.6631944444444444, "no_speech_prob": 0.050286274403333664}, {"id": 44, "seek": 16142, "start": 161.42, "end": 165.05999999999997, "text": " So basically that's what I'm doing.", "tokens": [50364, 407, 1936, 300, 311, 437, 286, 478, 884, 13, 50546], "temperature": 0.0, "avg_logprob": -0.16511305323186912, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.01970568299293518}, {"id": 45, "seek": 16142, "start": 165.05999999999997, "end": 167.7, "text": " Can you tell me a little bit about the focus of your research?", "tokens": [50546, 1664, 291, 980, 385, 257, 707, 857, 466, 264, 1879, 295, 428, 2132, 30, 50678], "temperature": 0.0, "avg_logprob": -0.16511305323186912, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.01970568299293518}, {"id": 46, "seek": 16142, "start": 167.7, "end": 170.61999999999998, "text": " I mostly study distributed machine learning.", "tokens": [50678, 286, 5240, 2979, 12631, 3479, 2539, 13, 50824], "temperature": 0.0, "avg_logprob": -0.16511305323186912, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.01970568299293518}, {"id": 47, "seek": 16142, "start": 170.61999999999998, "end": 172.54, "text": " So basically parallel computing.", "tokens": [50824, 407, 1936, 8952, 15866, 13, 50920], "temperature": 0.0, "avg_logprob": -0.16511305323186912, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.01970568299293518}, {"id": 48, "seek": 16142, "start": 172.54, "end": 177.01999999999998, "text": " Currently as the model becomes bigger and data also becomes bigger, so one GPU seems", "tokens": [50920, 19964, 382, 264, 2316, 3643, 3801, 293, 1412, 611, 3643, 3801, 11, 370, 472, 18407, 2544, 51144], "temperature": 0.0, "avg_logprob": -0.16511305323186912, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.01970568299293518}, {"id": 49, "seek": 16142, "start": 177.01999999999998, "end": 182.06, "text": " not enough to train our model or even do the so-called fine tuning these days.", "tokens": [51144, 406, 1547, 281, 3847, 527, 2316, 420, 754, 360, 264, 370, 12, 11880, 2489, 15164, 613, 1708, 13, 51396], "temperature": 0.0, "avg_logprob": -0.16511305323186912, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.01970568299293518}, {"id": 50, "seek": 16142, "start": 182.06, "end": 188.73999999999998, "text": " So my research is about how you can use more, utilize more CPU cores or more GPUs to accelerate", "tokens": [51396, 407, 452, 2132, 307, 466, 577, 291, 393, 764, 544, 11, 16117, 544, 13199, 24826, 420, 544, 18407, 82, 281, 21341, 51730], "temperature": 0.0, "avg_logprob": -0.16511305323186912, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.01970568299293518}, {"id": 51, "seek": 18874, "start": 188.74, "end": 193.14000000000001, "text": " your computing during training or model fine tuning.", "tokens": [50364, 428, 15866, 1830, 3097, 420, 2316, 2489, 15164, 13, 50584], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 52, "seek": 18874, "start": 193.14000000000001, "end": 198.46, "text": " But there the problem we care about is like when you use 1000 GPUs together, how can you", "tokens": [50584, 583, 456, 264, 1154, 321, 1127, 466, 307, 411, 562, 291, 764, 9714, 18407, 82, 1214, 11, 577, 393, 291, 50850], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 53, "seek": 18874, "start": 198.46, "end": 199.9, "text": " get 1000 times faster?", "tokens": [50850, 483, 9714, 1413, 4663, 30, 50922], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 54, "seek": 18874, "start": 199.9, "end": 202.22, "text": " So that's what we call scaling problem.", "tokens": [50922, 407, 300, 311, 437, 321, 818, 21589, 1154, 13, 51038], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 55, "seek": 18874, "start": 202.22, "end": 206.38, "text": " Now we are kind of doing kind of well for the smaller model.", "tokens": [51038, 823, 321, 366, 733, 295, 884, 733, 295, 731, 337, 264, 4356, 2316, 13, 51246], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 56, "seek": 18874, "start": 206.38, "end": 209.9, "text": " Let's say if you want to train a residual network or convolution network, it seems to", "tokens": [51246, 961, 311, 584, 498, 291, 528, 281, 3847, 257, 27980, 3209, 420, 45216, 3209, 11, 309, 2544, 281, 51422], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 57, "seek": 18874, "start": 209.9, "end": 210.9, "text": " be fine.", "tokens": [51422, 312, 2489, 13, 51472], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 58, "seek": 18874, "start": 210.9, "end": 212.98000000000002, "text": " We solve the scaling problem okay.", "tokens": [51472, 492, 5039, 264, 21589, 1154, 1392, 13, 51576], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 59, "seek": 18874, "start": 212.98000000000002, "end": 218.18, "text": " But for large language models, sometimes we need to go beyond what we call data parallelism.", "tokens": [51576, 583, 337, 2416, 2856, 5245, 11, 2171, 321, 643, 281, 352, 4399, 437, 321, 818, 1412, 8952, 1434, 13, 51836], "temperature": 0.0, "avg_logprob": -0.19142463130335655, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.3272383511066437}, {"id": 60, "seek": 21818, "start": 218.18, "end": 219.42000000000002, "text": " That makes the problem harder.", "tokens": [50364, 663, 1669, 264, 1154, 6081, 13, 50426], "temperature": 0.0, "avg_logprob": -0.09854720955464377, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.1292877197265625}, {"id": 61, "seek": 21818, "start": 219.42000000000002, "end": 221.9, "text": " So we are working hard on that recently.", "tokens": [50426, 407, 321, 366, 1364, 1152, 322, 300, 3938, 13, 50550], "temperature": 0.0, "avg_logprob": -0.09854720955464377, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.1292877197265625}, {"id": 62, "seek": 21818, "start": 221.9, "end": 227.38, "text": " So I have written a fair amount of Java code in my professional career and on one or two", "tokens": [50550, 407, 286, 362, 3720, 257, 3143, 2372, 295, 10745, 3089, 294, 452, 4843, 3988, 293, 322, 472, 420, 732, 50824], "temperature": 0.0, "avg_logprob": -0.09854720955464377, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.1292877197265625}, {"id": 63, "seek": 21818, "start": 227.38, "end": 232.02, "text": " occasions had a collaborator who knew about the underlying architecture of the JVM much", "tokens": [50824, 20641, 632, 257, 5091, 1639, 567, 2586, 466, 264, 14217, 9482, 295, 264, 508, 53, 44, 709, 51056], "temperature": 0.0, "avg_logprob": -0.09854720955464377, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.1292877197265625}, {"id": 64, "seek": 21818, "start": 232.02, "end": 236.54000000000002, "text": " better than I did and was able to go do some very fine tuned optimizations about the way", "tokens": [51056, 1101, 813, 286, 630, 293, 390, 1075, 281, 352, 360, 512, 588, 2489, 10870, 5028, 14455, 466, 264, 636, 51282], "temperature": 0.0, "avg_logprob": -0.09854720955464377, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.1292877197265625}, {"id": 65, "seek": 21818, "start": 236.54000000000002, "end": 240.14000000000001, "text": " garbage collection work to make everything just function a lot better.", "tokens": [51282, 14150, 5765, 589, 281, 652, 1203, 445, 2445, 257, 688, 1101, 13, 51462], "temperature": 0.0, "avg_logprob": -0.09854720955464377, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.1292877197265625}, {"id": 66, "seek": 21818, "start": 240.14000000000001, "end": 243.42000000000002, "text": " I don't have that skill set and I kind of don't want it.", "tokens": [51462, 286, 500, 380, 362, 300, 5389, 992, 293, 286, 733, 295, 500, 380, 528, 309, 13, 51626], "temperature": 0.0, "avg_logprob": -0.09854720955464377, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.1292877197265625}, {"id": 67, "seek": 21818, "start": 243.42000000000002, "end": 247.38, "text": " I'd rather just sort of do what I know there and let someone else handle that.", "tokens": [51626, 286, 1116, 2831, 445, 1333, 295, 360, 437, 286, 458, 456, 293, 718, 1580, 1646, 4813, 300, 13, 51824], "temperature": 0.0, "avg_logprob": -0.09854720955464377, "compression_ratio": 1.7106918238993711, "no_speech_prob": 0.1292877197265625}, {"id": 68, "seek": 24738, "start": 247.38, "end": 251.46, "text": " But then I try and carry that analogy over to deep learning where I am aware of things", "tokens": [50364, 583, 550, 286, 853, 293, 3985, 300, 21663, 670, 281, 2452, 2539, 689, 286, 669, 3650, 295, 721, 50568], "temperature": 0.0, "avg_logprob": -0.13447334806797867, "compression_ratio": 1.6915584415584415, "no_speech_prob": 0.014057652093470097}, {"id": 69, "seek": 24738, "start": 251.46, "end": 256.46, "text": " like batch normalization and the vanishing gradient and concepts like this or even the", "tokens": [50568, 411, 15245, 2710, 2144, 293, 264, 3161, 3807, 16235, 293, 10392, 411, 341, 420, 754, 264, 50818], "temperature": 0.0, "avg_logprob": -0.13447334806797867, "compression_ratio": 1.6915584415584415, "no_speech_prob": 0.014057652093470097}, {"id": 70, "seek": 24738, "start": 256.46, "end": 258.9, "text": " transformer architecture and how it works.", "tokens": [50818, 31782, 9482, 293, 577, 309, 1985, 13, 50940], "temperature": 0.0, "avg_logprob": -0.13447334806797867, "compression_ratio": 1.6915584415584415, "no_speech_prob": 0.014057652093470097}, {"id": 71, "seek": 24738, "start": 258.9, "end": 263.5, "text": " Do you think it's necessary that people who use deep learning know the fundamentals and", "tokens": [50940, 1144, 291, 519, 309, 311, 4818, 300, 561, 567, 764, 2452, 2539, 458, 264, 29505, 293, 51170], "temperature": 0.0, "avg_logprob": -0.13447334806797867, "compression_ratio": 1.6915584415584415, "no_speech_prob": 0.014057652093470097}, {"id": 72, "seek": 24738, "start": 263.5, "end": 267.86, "text": " the methods in that way or are we maybe entering a world where someone could just engineer", "tokens": [51170, 264, 7150, 294, 300, 636, 420, 366, 321, 1310, 11104, 257, 1002, 689, 1580, 727, 445, 11403, 51388], "temperature": 0.0, "avg_logprob": -0.13447334806797867, "compression_ratio": 1.6915584415584415, "no_speech_prob": 0.014057652093470097}, {"id": 73, "seek": 24738, "start": 267.86, "end": 269.62, "text": " on top of that technology?", "tokens": [51388, 322, 1192, 295, 300, 2899, 30, 51476], "temperature": 0.0, "avg_logprob": -0.13447334806797867, "compression_ratio": 1.6915584415584415, "no_speech_prob": 0.014057652093470097}, {"id": 74, "seek": 24738, "start": 269.62, "end": 270.62, "text": " Right.", "tokens": [51476, 1779, 13, 51526], "temperature": 0.0, "avg_logprob": -0.13447334806797867, "compression_ratio": 1.6915584415584415, "no_speech_prob": 0.014057652093470097}, {"id": 75, "seek": 24738, "start": 270.62, "end": 276.86, "text": " I think that's pretty kind of like a visionary problem and we're actually working very hard", "tokens": [51526, 286, 519, 300, 311, 1238, 733, 295, 411, 257, 49442, 1154, 293, 321, 434, 767, 1364, 588, 1152, 51838], "temperature": 0.0, "avg_logprob": -0.13447334806797867, "compression_ratio": 1.6915584415584415, "no_speech_prob": 0.014057652093470097}, {"id": 76, "seek": 27686, "start": 276.94, "end": 277.94, "text": " on that.", "tokens": [50368, 322, 300, 13, 50418], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 77, "seek": 27686, "start": 277.94, "end": 283.14, "text": " So as I just mentioned, if I do not have a system like knowledge, I just work on fundamental", "tokens": [50418, 407, 382, 286, 445, 2835, 11, 498, 286, 360, 406, 362, 257, 1185, 411, 3601, 11, 286, 445, 589, 322, 8088, 50678], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 78, "seek": 27686, "start": 283.14, "end": 287.42, "text": " science, let's say, and I want to use the foundation model or kind of like deep learning", "tokens": [50678, 3497, 11, 718, 311, 584, 11, 293, 286, 528, 281, 764, 264, 7030, 2316, 420, 733, 295, 411, 2452, 2539, 50892], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 79, "seek": 27686, "start": 287.42, "end": 289.54, "text": " to kind of like power my research.", "tokens": [50892, 281, 733, 295, 411, 1347, 452, 2132, 13, 50998], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 80, "seek": 27686, "start": 289.54, "end": 292.78000000000003, "text": " It's basically kind of like impossible in the past.", "tokens": [50998, 467, 311, 1936, 733, 295, 411, 6243, 294, 264, 1791, 13, 51160], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 81, "seek": 27686, "start": 292.78000000000003, "end": 296.74, "text": " But now we can kind of like have PyTorch, like TensorFlow, it seems to become easier,", "tokens": [51160, 583, 586, 321, 393, 733, 295, 411, 362, 9953, 51, 284, 339, 11, 411, 37624, 11, 309, 2544, 281, 1813, 3571, 11, 51358], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 82, "seek": 27686, "start": 296.74, "end": 297.74, "text": " but still very hard.", "tokens": [51358, 457, 920, 588, 1152, 13, 51408], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 83, "seek": 27686, "start": 297.74, "end": 302.46000000000004, "text": " Let's say if I were even given, let's say, like 1000 GPU from Microsoft, I don't even", "tokens": [51408, 961, 311, 584, 498, 286, 645, 754, 2212, 11, 718, 311, 584, 11, 411, 9714, 18407, 490, 8116, 11, 286, 500, 380, 754, 51644], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 84, "seek": 27686, "start": 302.46000000000004, "end": 304.74, "text": " know how to use the 1000 GPU together.", "tokens": [51644, 458, 577, 281, 764, 264, 9714, 18407, 1214, 13, 51758], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 85, "seek": 27686, "start": 304.74, "end": 305.90000000000003, "text": " So that's the problem.", "tokens": [51758, 407, 300, 311, 264, 1154, 13, 51816], "temperature": 0.0, "avg_logprob": -0.20878907521565757, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.20163939893245697}, {"id": 86, "seek": 30590, "start": 305.9, "end": 310.62, "text": " We are kind of like solving, kind of like come up with another layer in between from", "tokens": [50364, 492, 366, 733, 295, 411, 12606, 11, 733, 295, 411, 808, 493, 365, 1071, 4583, 294, 1296, 490, 50600], "temperature": 0.0, "avg_logprob": -0.20759413146972655, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.016396893188357353}, {"id": 87, "seek": 30590, "start": 310.62, "end": 314.58, "text": " application people and engineer and also the hardware.", "tokens": [50600, 3861, 561, 293, 11403, 293, 611, 264, 8837, 13, 50798], "temperature": 0.0, "avg_logprob": -0.20759413146972655, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.016396893188357353}, {"id": 88, "seek": 30590, "start": 314.58, "end": 315.58, "text": " So that's what we are doing.", "tokens": [50798, 407, 300, 311, 437, 321, 366, 884, 13, 50848], "temperature": 0.0, "avg_logprob": -0.20759413146972655, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.016396893188357353}, {"id": 89, "seek": 30590, "start": 315.58, "end": 321.7, "text": " Hopefully we can automatically deploy your code on the hardware in the optimized way", "tokens": [50848, 10429, 321, 393, 6772, 7274, 428, 3089, 322, 264, 8837, 294, 264, 26941, 636, 51154], "temperature": 0.0, "avg_logprob": -0.20759413146972655, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.016396893188357353}, {"id": 90, "seek": 30590, "start": 321.7, "end": 323.09999999999997, "text": " or in kind of like an optimal way.", "tokens": [51154, 420, 294, 733, 295, 411, 364, 16252, 636, 13, 51224], "temperature": 0.0, "avg_logprob": -0.20759413146972655, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.016396893188357353}, {"id": 91, "seek": 30590, "start": 323.09999999999997, "end": 328.09999999999997, "text": " Yeah, I know it won't be the focus of our main discussion today for the paper, Cuttlefish,", "tokens": [51224, 865, 11, 286, 458, 309, 1582, 380, 312, 264, 1879, 295, 527, 2135, 5017, 965, 337, 264, 3035, 11, 9431, 10972, 11608, 11, 51474], "temperature": 0.0, "avg_logprob": -0.20759413146972655, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.016396893188357353}, {"id": 92, "seek": 30590, "start": 328.09999999999997, "end": 329.85999999999996, "text": " that I invite you to talk about.", "tokens": [51474, 300, 286, 7980, 291, 281, 751, 466, 13, 51562], "temperature": 0.0, "avg_logprob": -0.20759413146972655, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.016396893188357353}, {"id": 93, "seek": 30590, "start": 329.85999999999996, "end": 334.23999999999995, "text": " But before we get into that, could you just reference a little bit about the hardware?", "tokens": [51562, 583, 949, 321, 483, 666, 300, 11, 727, 291, 445, 6408, 257, 707, 857, 466, 264, 8837, 30, 51781], "temperature": 0.0, "avg_logprob": -0.20759413146972655, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.016396893188357353}, {"id": 94, "seek": 33424, "start": 334.24, "end": 337.2, "text": " Is that like FPGAs or what are you thinking there?", "tokens": [50364, 1119, 300, 411, 36655, 38, 10884, 420, 437, 366, 291, 1953, 456, 30, 50512], "temperature": 0.0, "avg_logprob": -0.25874482859735903, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.11269665509462357}, {"id": 95, "seek": 33424, "start": 337.2, "end": 344.40000000000003, "text": " Actually, yeah, I know the world is currently GPU dominated or TPU dominated, but we're", "tokens": [50512, 5135, 11, 1338, 11, 286, 458, 264, 1002, 307, 4362, 18407, 23755, 420, 314, 8115, 23755, 11, 457, 321, 434, 50872], "temperature": 0.0, "avg_logprob": -0.25874482859735903, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.11269665509462357}, {"id": 96, "seek": 33424, "start": 344.40000000000003, "end": 347.08, "text": " actually thinking about a wider picture.", "tokens": [50872, 767, 1953, 466, 257, 11842, 3036, 13, 51006], "temperature": 0.0, "avg_logprob": -0.25874482859735903, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.11269665509462357}, {"id": 97, "seek": 33424, "start": 347.08, "end": 351.3, "text": " So there are a whole bunch of like used hardware everywhere, right?", "tokens": [51006, 407, 456, 366, 257, 1379, 3840, 295, 411, 1143, 8837, 5315, 11, 558, 30, 51217], "temperature": 0.0, "avg_logprob": -0.25874482859735903, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.11269665509462357}, {"id": 98, "seek": 33424, "start": 351.3, "end": 356.04, "text": " Even in the AWS cloud, there are data centers, there are a lot of like used GPUs, also the", "tokens": [51217, 2754, 294, 264, 17650, 4588, 11, 456, 366, 1412, 10898, 11, 456, 366, 257, 688, 295, 411, 1143, 18407, 82, 11, 611, 264, 51454], "temperature": 0.0, "avg_logprob": -0.25874482859735903, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.11269665509462357}, {"id": 99, "seek": 33424, "start": 356.04, "end": 357.44, "text": " community hardware.", "tokens": [51454, 1768, 8837, 13, 51524], "temperature": 0.0, "avg_logprob": -0.25874482859735903, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.11269665509462357}, {"id": 100, "seek": 33424, "start": 357.44, "end": 364.04, "text": " So one of my research goals is to try to combine everything together, kind of like reuse the", "tokens": [51524, 407, 472, 295, 452, 2132, 5493, 307, 281, 853, 281, 10432, 1203, 1214, 11, 733, 295, 411, 26225, 264, 51854], "temperature": 0.0, "avg_logprob": -0.25874482859735903, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.11269665509462357}, {"id": 101, "seek": 36404, "start": 364.04, "end": 367.24, "text": " machine, everything, your CPU cores.", "tokens": [50364, 3479, 11, 1203, 11, 428, 13199, 24826, 13, 50524], "temperature": 0.0, "avg_logprob": -0.24510933062352172, "compression_ratio": 1.6356589147286822, "no_speech_prob": 0.020957333967089653}, {"id": 102, "seek": 36404, "start": 367.24, "end": 373.12, "text": " Then there, like if you kind of calculate the theoretical flaws, that's actually comparable", "tokens": [50524, 1396, 456, 11, 411, 498, 291, 733, 295, 8873, 264, 20864, 27108, 11, 300, 311, 767, 25323, 50818], "temperature": 0.0, "avg_logprob": -0.24510933062352172, "compression_ratio": 1.6356589147286822, "no_speech_prob": 0.020957333967089653}, {"id": 103, "seek": 36404, "start": 373.12, "end": 376.88, "text": " if we use as much, you know, those used hardware as possible.", "tokens": [50818, 498, 321, 764, 382, 709, 11, 291, 458, 11, 729, 1143, 8837, 382, 1944, 13, 51006], "temperature": 0.0, "avg_logprob": -0.24510933062352172, "compression_ratio": 1.6356589147286822, "no_speech_prob": 0.020957333967089653}, {"id": 104, "seek": 36404, "start": 376.88, "end": 379.08000000000004, "text": " It's comparable to the A100.", "tokens": [51006, 467, 311, 25323, 281, 264, 316, 6879, 13, 51116], "temperature": 0.0, "avg_logprob": -0.24510933062352172, "compression_ratio": 1.6356589147286822, "no_speech_prob": 0.020957333967089653}, {"id": 105, "seek": 36404, "start": 379.08000000000004, "end": 382.68, "text": " But the problem is there is how you can use them efficiently together.", "tokens": [51116, 583, 264, 1154, 307, 456, 307, 577, 291, 393, 764, 552, 19621, 1214, 13, 51296], "temperature": 0.0, "avg_logprob": -0.24510933062352172, "compression_ratio": 1.6356589147286822, "no_speech_prob": 0.020957333967089653}, {"id": 106, "seek": 36404, "start": 382.68, "end": 387.48, "text": " So that's what some of my past work and my future work will try to tackle.", "tokens": [51296, 407, 300, 311, 437, 512, 295, 452, 1791, 589, 293, 452, 2027, 589, 486, 853, 281, 14896, 13, 51536], "temperature": 0.0, "avg_logprob": -0.24510933062352172, "compression_ratio": 1.6356589147286822, "no_speech_prob": 0.020957333967089653}, {"id": 107, "seek": 36404, "start": 387.48, "end": 389.72, "text": " Well, you know, theoretically you can solve this, right?", "tokens": [51536, 1042, 11, 291, 458, 11, 29400, 291, 393, 5039, 341, 11, 558, 30, 51648], "temperature": 0.0, "avg_logprob": -0.24510933062352172, "compression_ratio": 1.6356589147286822, "no_speech_prob": 0.020957333967089653}, {"id": 108, "seek": 38972, "start": 389.72, "end": 395.44000000000005, "text": " You can build a very, very fancy cost model and you solve how you can parallelize your", "tokens": [50364, 509, 393, 1322, 257, 588, 11, 588, 10247, 2063, 2316, 293, 291, 5039, 577, 291, 393, 8952, 1125, 428, 50650], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 109, "seek": 38972, "start": 395.44000000000005, "end": 398.20000000000005, "text": " computation across all the hardware.", "tokens": [50650, 24903, 2108, 439, 264, 8837, 13, 50788], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 110, "seek": 38972, "start": 398.20000000000005, "end": 402.68, "text": " It's basically a graph partitioning problem, but theoretically we can solve that.", "tokens": [50788, 467, 311, 1936, 257, 4295, 24808, 278, 1154, 11, 457, 29400, 321, 393, 5039, 300, 13, 51012], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 111, "seek": 38972, "start": 402.68, "end": 404.6, "text": " It's kind of like the engineering problem.", "tokens": [51012, 467, 311, 733, 295, 411, 264, 7043, 1154, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 112, "seek": 38972, "start": 404.6, "end": 406.84000000000003, "text": " How can we like scheduling things well?", "tokens": [51108, 1012, 393, 321, 411, 29055, 721, 731, 30, 51220], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 113, "seek": 38972, "start": 406.84000000000003, "end": 408.8, "text": " How can we use them cleverly?", "tokens": [51220, 1012, 393, 321, 764, 552, 13494, 356, 30, 51318], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 114, "seek": 38972, "start": 408.8, "end": 410.28000000000003, "text": " And how can we even handle the fault tolerance?", "tokens": [51318, 400, 577, 393, 321, 754, 4813, 264, 7441, 23368, 30, 51392], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 115, "seek": 38972, "start": 410.28000000000003, "end": 415.56, "text": " So like when we have a whole bunch of hardware together, it's very likely like some of them", "tokens": [51392, 407, 411, 562, 321, 362, 257, 1379, 3840, 295, 8837, 1214, 11, 309, 311, 588, 3700, 411, 512, 295, 552, 51656], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 116, "seek": 38972, "start": 415.56, "end": 418.64000000000004, "text": " will fail and it seems like that.", "tokens": [51656, 486, 3061, 293, 309, 2544, 411, 300, 13, 51810], "temperature": 0.0, "avg_logprob": -0.2242851564961095, "compression_ratio": 1.7446808510638299, "no_speech_prob": 0.4603552520275116}, {"id": 117, "seek": 41864, "start": 418.68, "end": 420.0, "text": " Yeah, that's definitely possible.", "tokens": [50366, 865, 11, 300, 311, 2138, 1944, 13, 50432], "temperature": 0.0, "avg_logprob": -0.18514463978428994, "compression_ratio": 1.5381944444444444, "no_speech_prob": 0.03619978204369545}, {"id": 118, "seek": 41864, "start": 420.0, "end": 422.91999999999996, "text": " And I'm devoting my time to work on that.", "tokens": [50432, 400, 286, 478, 13697, 278, 452, 565, 281, 589, 322, 300, 13, 50578], "temperature": 0.0, "avg_logprob": -0.18514463978428994, "compression_ratio": 1.5381944444444444, "no_speech_prob": 0.03619978204369545}, {"id": 119, "seek": 41864, "start": 423.28, "end": 428.2, "text": " Well, if we look back in recent history, it's not like NLP has changed overnight.", "tokens": [50596, 1042, 11, 498, 321, 574, 646, 294, 5162, 2503, 11, 309, 311, 406, 411, 426, 45196, 575, 3105, 13935, 13, 50842], "temperature": 0.0, "avg_logprob": -0.18514463978428994, "compression_ratio": 1.5381944444444444, "no_speech_prob": 0.03619978204369545}, {"id": 120, "seek": 41864, "start": 428.2, "end": 431.56, "text": " I mean, maybe it did in some ways when some of these amazing models came out.", "tokens": [50842, 286, 914, 11, 1310, 309, 630, 294, 512, 2098, 562, 512, 295, 613, 2243, 5245, 1361, 484, 13, 51010], "temperature": 0.0, "avg_logprob": -0.18514463978428994, "compression_ratio": 1.5381944444444444, "no_speech_prob": 0.03619978204369545}, {"id": 121, "seek": 41864, "start": 431.56, "end": 436.64, "text": " But we can look back to like Word2vec as a pretty impressive technology that's, I guess,", "tokens": [51010, 583, 321, 393, 574, 646, 281, 411, 8725, 17, 303, 66, 382, 257, 1238, 8992, 2899, 300, 311, 11, 286, 2041, 11, 51264], "temperature": 0.0, "avg_logprob": -0.18514463978428994, "compression_ratio": 1.5381944444444444, "no_speech_prob": 0.03619978204369545}, {"id": 122, "seek": 41864, "start": 436.64, "end": 437.52, "text": " a decade old.", "tokens": [51264, 257, 10378, 1331, 13, 51308], "temperature": 0.0, "avg_logprob": -0.18514463978428994, "compression_ratio": 1.5381944444444444, "no_speech_prob": 0.03619978204369545}, {"id": 123, "seek": 41864, "start": 437.96, "end": 442.36, "text": " At what point or was there a point when you first took notice and said, wow, something's", "tokens": [51330, 1711, 437, 935, 420, 390, 456, 257, 935, 562, 291, 700, 1890, 3449, 293, 848, 11, 6076, 11, 746, 311, 51550], "temperature": 0.0, "avg_logprob": -0.18514463978428994, "compression_ratio": 1.5381944444444444, "no_speech_prob": 0.03619978204369545}, {"id": 124, "seek": 41864, "start": 442.36, "end": 443.03999999999996, "text": " happening here?", "tokens": [51550, 2737, 510, 30, 51584], "temperature": 0.0, "avg_logprob": -0.18514463978428994, "compression_ratio": 1.5381944444444444, "no_speech_prob": 0.03619978204369545}, {"id": 125, "seek": 44304, "start": 443.24, "end": 450.88, "text": " Mostly, when I joined this community, it's mostly there's no NLP scaling problem yet,", "tokens": [50374, 29035, 11, 562, 286, 6869, 341, 1768, 11, 309, 311, 5240, 456, 311, 572, 426, 45196, 21589, 1154, 1939, 11, 50756], "temperature": 0.0, "avg_logprob": -0.3185726331627887, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012616266496479511}, {"id": 126, "seek": 44304, "start": 450.88, "end": 451.48, "text": " I would say.", "tokens": [50756, 286, 576, 584, 13, 50786], "temperature": 0.0, "avg_logprob": -0.3185726331627887, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012616266496479511}, {"id": 127, "seek": 44304, "start": 451.72, "end": 458.24, "text": " So at that time, people are excited about like VGG, how you can scale VGG on ImageNet", "tokens": [50798, 407, 412, 300, 565, 11, 561, 366, 2919, 466, 411, 691, 27561, 11, 577, 291, 393, 4373, 691, 27561, 322, 29903, 31890, 51124], "temperature": 0.0, "avg_logprob": -0.3185726331627887, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012616266496479511}, {"id": 128, "seek": 44304, "start": 458.24, "end": 458.88, "text": " training.", "tokens": [51124, 3097, 13, 51156], "temperature": 0.0, "avg_logprob": -0.3185726331627887, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012616266496479511}, {"id": 129, "seek": 44304, "start": 458.88, "end": 463.76, "text": " So that already becomes a problem because the ImageNet is huge.", "tokens": [51156, 407, 300, 1217, 3643, 257, 1154, 570, 264, 29903, 31890, 307, 2603, 13, 51400], "temperature": 0.0, "avg_logprob": -0.3185726331627887, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012616266496479511}, {"id": 130, "seek": 44304, "start": 463.76, "end": 466.72, "text": " And also, there's a whole bunch of images there.", "tokens": [51400, 400, 611, 11, 456, 311, 257, 1379, 3840, 295, 5267, 456, 13, 51548], "temperature": 0.0, "avg_logprob": -0.3185726331627887, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012616266496479511}, {"id": 131, "seek": 44304, "start": 466.72, "end": 471.40000000000003, "text": " There's currently one, if I remember correctly, it's kind of like one million images inside", "tokens": [51548, 821, 311, 4362, 472, 11, 498, 286, 1604, 8944, 11, 309, 311, 733, 295, 411, 472, 2459, 5267, 1854, 51782], "temperature": 0.0, "avg_logprob": -0.3185726331627887, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012616266496479511}, {"id": 132, "seek": 44304, "start": 471.40000000000003, "end": 472.16, "text": " of the data set.", "tokens": [51782, 295, 264, 1412, 992, 13, 51820], "temperature": 0.0, "avg_logprob": -0.3185726331627887, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012616266496479511}, {"id": 133, "seek": 47216, "start": 472.16, "end": 476.48, "text": " So even kind of like one epoch, now it's called epoch of training, it takes a whole", "tokens": [50364, 407, 754, 733, 295, 411, 472, 30992, 339, 11, 586, 309, 311, 1219, 30992, 339, 295, 3097, 11, 309, 2516, 257, 1379, 50580], "temperature": 0.0, "avg_logprob": -0.14834979484821187, "compression_ratio": 1.699248120300752, "no_speech_prob": 0.0004952846793457866}, {"id": 134, "seek": 47216, "start": 476.48, "end": 477.16, "text": " bunch of time.", "tokens": [50580, 3840, 295, 565, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14834979484821187, "compression_ratio": 1.699248120300752, "no_speech_prob": 0.0004952846793457866}, {"id": 135, "seek": 47216, "start": 477.52000000000004, "end": 483.08000000000004, "text": " So at that time, we were trying to optimize things like how can we make the model go", "tokens": [50632, 407, 412, 300, 565, 11, 321, 645, 1382, 281, 19719, 721, 411, 577, 393, 321, 652, 264, 2316, 352, 50910], "temperature": 0.0, "avg_logprob": -0.14834979484821187, "compression_ratio": 1.699248120300752, "no_speech_prob": 0.0004952846793457866}, {"id": 136, "seek": 47216, "start": 483.08000000000004, "end": 488.6, "text": " pass through the data passes as many as possible and as fast as possible.", "tokens": [50910, 1320, 807, 264, 1412, 11335, 382, 867, 382, 1944, 293, 382, 2370, 382, 1944, 13, 51186], "temperature": 0.0, "avg_logprob": -0.14834979484821187, "compression_ratio": 1.699248120300752, "no_speech_prob": 0.0004952846793457866}, {"id": 137, "seek": 47216, "start": 488.76000000000005, "end": 493.48, "text": " So I would say, you know, like after ImageNet challenging the benchmark, things start to", "tokens": [51194, 407, 286, 576, 584, 11, 291, 458, 11, 411, 934, 29903, 31890, 7595, 264, 18927, 11, 721, 722, 281, 51430], "temperature": 0.0, "avg_logprob": -0.14834979484821187, "compression_ratio": 1.699248120300752, "no_speech_prob": 0.0004952846793457866}, {"id": 138, "seek": 47216, "start": 493.48, "end": 494.32000000000005, "text": " become different.", "tokens": [51430, 1813, 819, 13, 51472], "temperature": 0.0, "avg_logprob": -0.14834979484821187, "compression_ratio": 1.699248120300752, "no_speech_prob": 0.0004952846793457866}, {"id": 139, "seek": 47216, "start": 494.32000000000005, "end": 499.36, "text": " Like all of the people are starting to care about how to make things faster and how can", "tokens": [51472, 1743, 439, 295, 264, 561, 366, 2891, 281, 1127, 466, 577, 281, 652, 721, 4663, 293, 577, 393, 51724], "temperature": 0.0, "avg_logprob": -0.14834979484821187, "compression_ratio": 1.699248120300752, "no_speech_prob": 0.0004952846793457866}, {"id": 140, "seek": 49936, "start": 499.36, "end": 504.36, "text": " we optimize all the software stack to try to keep the top of the benchmark as much as", "tokens": [50364, 321, 19719, 439, 264, 4722, 8630, 281, 853, 281, 1066, 264, 1192, 295, 264, 18927, 382, 709, 382, 50614], "temperature": 0.0, "avg_logprob": -0.21632282908369854, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.04332888498902321}, {"id": 141, "seek": 49936, "start": 504.36, "end": 504.84000000000003, "text": " possible.", "tokens": [50614, 1944, 13, 50638], "temperature": 0.0, "avg_logprob": -0.21632282908369854, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.04332888498902321}, {"id": 142, "seek": 49936, "start": 505.12, "end": 509.72, "text": " And now we have some other benchmark, you know, kind of like people are hitting the", "tokens": [50652, 400, 586, 321, 362, 512, 661, 18927, 11, 291, 458, 11, 733, 295, 411, 561, 366, 8850, 264, 50882], "temperature": 0.0, "avg_logprob": -0.21632282908369854, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.04332888498902321}, {"id": 143, "seek": 49936, "start": 509.72, "end": 512.6, "text": " let's say VKUNA leaderboard or Puggingface leaderboard.", "tokens": [50882, 718, 311, 584, 691, 42, 3979, 32, 5263, 3787, 420, 430, 697, 3249, 2868, 5263, 3787, 13, 51026], "temperature": 0.0, "avg_logprob": -0.21632282908369854, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.04332888498902321}, {"id": 144, "seek": 49936, "start": 512.88, "end": 517.2, "text": " It seems to be the newer version of ImageNet in this current era.", "tokens": [51040, 467, 2544, 281, 312, 264, 17628, 3037, 295, 29903, 31890, 294, 341, 2190, 4249, 13, 51256], "temperature": 0.0, "avg_logprob": -0.21632282908369854, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.04332888498902321}, {"id": 145, "seek": 49936, "start": 517.76, "end": 523.64, "text": " So I've heard some reports about the cost of running some of these large language models.", "tokens": [51284, 407, 286, 600, 2198, 512, 7122, 466, 264, 2063, 295, 2614, 512, 295, 613, 2416, 2856, 5245, 13, 51578], "temperature": 0.0, "avg_logprob": -0.21632282908369854, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.04332888498902321}, {"id": 146, "seek": 49936, "start": 523.64, "end": 528.72, "text": " Like maybe it costs one penny for a thousand requests of chat GPT or something like that.", "tokens": [51578, 1743, 1310, 309, 5497, 472, 24178, 337, 257, 4714, 12475, 295, 5081, 26039, 51, 420, 746, 411, 300, 13, 51832], "temperature": 0.0, "avg_logprob": -0.21632282908369854, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.04332888498902321}, {"id": 147, "seek": 52872, "start": 529.12, "end": 534.2, "text": " And it sounds so low when you describe it as a commodity, but actually in some sense,", "tokens": [50384, 400, 309, 3263, 370, 2295, 562, 291, 6786, 309, 382, 257, 29125, 11, 457, 767, 294, 512, 2020, 11, 50638], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 148, "seek": 52872, "start": 534.44, "end": 538.28, "text": " maybe it's expensive because we're doing things inefficiently or something along those", "tokens": [50650, 1310, 309, 311, 5124, 570, 321, 434, 884, 721, 43495, 356, 420, 746, 2051, 729, 50842], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 149, "seek": 52872, "start": 538.28, "end": 538.76, "text": " lines.", "tokens": [50842, 3876, 13, 50866], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 150, "seek": 52872, "start": 538.88, "end": 539.2, "text": " Yeah.", "tokens": [50872, 865, 13, 50888], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 151, "seek": 52872, "start": 539.24, "end": 542.9200000000001, "text": " Where are the opportunities to make efficiency gains here?", "tokens": [50890, 2305, 366, 264, 4786, 281, 652, 10493, 16823, 510, 30, 51074], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 152, "seek": 52872, "start": 543.44, "end": 543.6800000000001, "text": " Right.", "tokens": [51100, 1779, 13, 51112], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 153, "seek": 52872, "start": 543.6800000000001, "end": 549.84, "text": " So kind of in terms of the data center computing, I think it's if we can, everything comes", "tokens": [51112, 407, 733, 295, 294, 2115, 295, 264, 1412, 3056, 15866, 11, 286, 519, 309, 311, 498, 321, 393, 11, 1203, 1487, 51420], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 154, "seek": 52872, "start": 549.84, "end": 553.2, "text": " through the most advanced GPU.", "tokens": [51420, 807, 264, 881, 7339, 18407, 13, 51588], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 155, "seek": 52872, "start": 553.6, "end": 557.6, "text": " I think that has been optimized quite well, actually.", "tokens": [51608, 286, 519, 300, 575, 668, 26941, 1596, 731, 11, 767, 13, 51808], "temperature": 0.0, "avg_logprob": -0.22686776408442744, "compression_ratio": 1.5583941605839415, "no_speech_prob": 0.0013040235498920083}, {"id": 156, "seek": 55760, "start": 557.72, "end": 562.24, "text": " So in the sense that people may not even want to use, there's a whole bunch of different", "tokens": [50370, 407, 294, 264, 2020, 300, 561, 815, 406, 754, 528, 281, 764, 11, 456, 311, 257, 1379, 3840, 295, 819, 50596], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 157, "seek": 55760, "start": 562.24, "end": 563.08, "text": " parallelism, right?", "tokens": [50596, 8952, 1434, 11, 558, 30, 50638], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 158, "seek": 55760, "start": 563.08, "end": 567.0, "text": " So we have data parallelism model and also the pipeline parallelism.", "tokens": [50638, 407, 321, 362, 1412, 8952, 1434, 2316, 293, 611, 264, 15517, 8952, 1434, 13, 50834], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 159, "seek": 55760, "start": 567.0400000000001, "end": 568.52, "text": " So different of them thing.", "tokens": [50836, 407, 819, 295, 552, 551, 13, 50910], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 160, "seek": 55760, "start": 568.76, "end": 573.12, "text": " And the people have realized that when you can use them together, you can reach to some", "tokens": [50922, 400, 264, 561, 362, 5334, 300, 562, 291, 393, 764, 552, 1214, 11, 291, 393, 2524, 281, 512, 51140], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 161, "seek": 55760, "start": 573.12, "end": 574.76, "text": " sort of like optimal performance.", "tokens": [51140, 1333, 295, 411, 16252, 3389, 13, 51222], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 162, "seek": 55760, "start": 575.08, "end": 579.0400000000001, "text": " But now in the data center setting, I don't think people want to even use that.", "tokens": [51238, 583, 586, 294, 264, 1412, 3056, 3287, 11, 286, 500, 380, 519, 561, 528, 281, 754, 764, 300, 13, 51436], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 163, "seek": 55760, "start": 579.0400000000001, "end": 584.64, "text": " They just use the 4D shard data parallel released by it's also called Deep Speed Zero", "tokens": [51436, 814, 445, 764, 264, 1017, 35, 402, 515, 1412, 8952, 4736, 538, 309, 311, 611, 1219, 14895, 18774, 17182, 51716], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 164, "seek": 55760, "start": 584.64, "end": 586.08, "text": " Stage 3, but whatever.", "tokens": [51716, 25907, 805, 11, 457, 2035, 13, 51788], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 165, "seek": 55760, "start": 586.36, "end": 587.28, "text": " They're just using that.", "tokens": [51802, 814, 434, 445, 1228, 300, 13, 51848], "temperature": 0.0, "avg_logprob": -0.22603557126741883, "compression_ratio": 1.762214983713355, "no_speech_prob": 0.0001767100184224546}, {"id": 166, "seek": 58728, "start": 587.28, "end": 590.64, "text": " It's because the hardware is highly optimized in the data center.", "tokens": [50364, 467, 311, 570, 264, 8837, 307, 5405, 26941, 294, 264, 1412, 3056, 13, 50532], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 167, "seek": 58728, "start": 590.8399999999999, "end": 593.12, "text": " The kind of cross node bandwidth is high enough.", "tokens": [50542, 440, 733, 295, 3278, 9984, 23647, 307, 1090, 1547, 13, 50656], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 168, "seek": 58728, "start": 593.4399999999999, "end": 595.3199999999999, "text": " So it's fine.", "tokens": [50672, 407, 309, 311, 2489, 13, 50766], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 169, "seek": 58728, "start": 595.3199999999999, "end": 597.0799999999999, "text": " So everything is optimized quite well.", "tokens": [50766, 407, 1203, 307, 26941, 1596, 731, 13, 50854], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 170, "seek": 58728, "start": 597.24, "end": 601.4399999999999, "text": " But when we consider if we want to scale things out of the data center, right?", "tokens": [50862, 583, 562, 321, 1949, 498, 321, 528, 281, 4373, 721, 484, 295, 264, 1412, 3056, 11, 558, 30, 51072], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 171, "seek": 58728, "start": 601.4399999999999, "end": 605.1999999999999, "text": " We want to combine all the real hardware or the community hardware.", "tokens": [51072, 492, 528, 281, 10432, 439, 264, 957, 8837, 420, 264, 1768, 8837, 13, 51260], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 172, "seek": 58728, "start": 605.48, "end": 606.36, "text": " So that's different.", "tokens": [51274, 407, 300, 311, 819, 13, 51318], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 173, "seek": 58728, "start": 606.36, "end": 608.88, "text": " So the communication can be really the bottleneck there.", "tokens": [51318, 407, 264, 6101, 393, 312, 534, 264, 44641, 547, 456, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 174, "seek": 58728, "start": 609.0799999999999, "end": 613.88, "text": " So we consider there are two clusters located on different, one on the East Coast, one on", "tokens": [51454, 407, 321, 1949, 456, 366, 732, 23313, 6870, 322, 819, 11, 472, 322, 264, 6747, 14960, 11, 472, 322, 51694], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 175, "seek": 58728, "start": 613.88, "end": 614.6, "text": " the West Coast.", "tokens": [51694, 264, 4055, 14960, 13, 51730], "temperature": 0.0, "avg_logprob": -0.2103800435704509, "compression_ratio": 1.8308823529411764, "no_speech_prob": 0.0001106046256609261}, {"id": 176, "seek": 61460, "start": 614.9200000000001, "end": 617.84, "text": " So even one communication can take quite a long time.", "tokens": [50380, 407, 754, 472, 6101, 393, 747, 1596, 257, 938, 565, 13, 50526], "temperature": 0.0, "avg_logprob": -0.18230445168235085, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.0028883067425340414}, {"id": 177, "seek": 61460, "start": 618.48, "end": 619.64, "text": " So how can we do that?", "tokens": [50558, 407, 577, 393, 321, 360, 300, 30, 50616], "temperature": 0.0, "avg_logprob": -0.18230445168235085, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.0028883067425340414}, {"id": 178, "seek": 61460, "start": 619.64, "end": 625.4, "text": " How can we reduce the communication among there and try to do less communication as", "tokens": [50616, 1012, 393, 321, 5407, 264, 6101, 3654, 456, 293, 853, 281, 360, 1570, 6101, 382, 50904], "temperature": 0.0, "avg_logprob": -0.18230445168235085, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.0028883067425340414}, {"id": 179, "seek": 61460, "start": 625.4, "end": 628.32, "text": " much as possible, focus more on local computing?", "tokens": [50904, 709, 382, 1944, 11, 1879, 544, 322, 2654, 15866, 30, 51050], "temperature": 0.0, "avg_logprob": -0.18230445168235085, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.0028883067425340414}, {"id": 180, "seek": 61460, "start": 628.64, "end": 631.24, "text": " Yeah, that's I think where the opportunity is.", "tokens": [51066, 865, 11, 300, 311, 286, 519, 689, 264, 2650, 307, 13, 51196], "temperature": 0.0, "avg_logprob": -0.18230445168235085, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.0028883067425340414}, {"id": 181, "seek": 61460, "start": 632.12, "end": 637.36, "text": " Well, there's a popular technique I've heard called LoRa, the low rank approximation,", "tokens": [51240, 1042, 11, 456, 311, 257, 3743, 6532, 286, 600, 2198, 1219, 6130, 41873, 11, 264, 2295, 6181, 28023, 11, 51502], "temperature": 0.0, "avg_logprob": -0.18230445168235085, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.0028883067425340414}, {"id": 182, "seek": 61460, "start": 637.36, "end": 643.12, "text": " where I guess my, you can correct me if I'm wrong, but my understanding of it is take", "tokens": [51502, 689, 286, 2041, 452, 11, 291, 393, 3006, 385, 498, 286, 478, 2085, 11, 457, 452, 3701, 295, 309, 307, 747, 51790], "temperature": 0.0, "avg_logprob": -0.18230445168235085, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.0028883067425340414}, {"id": 183, "seek": 64312, "start": 643.12, "end": 648.48, "text": " the big model that's been trained in some expensive and maybe inefficient way and", "tokens": [50364, 264, 955, 2316, 300, 311, 668, 8895, 294, 512, 5124, 293, 1310, 43495, 636, 293, 50632], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 184, "seek": 64312, "start": 648.48, "end": 652.16, "text": " then find a way to compress it or reduce it or something like that.", "tokens": [50632, 550, 915, 257, 636, 281, 14778, 309, 420, 5407, 309, 420, 746, 411, 300, 13, 50816], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 185, "seek": 64312, "start": 652.2, "end": 652.64, "text": " Yeah.", "tokens": [50818, 865, 13, 50840], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 186, "seek": 64312, "start": 652.72, "end": 653.24, "text": " Yeah.", "tokens": [50844, 865, 13, 50870], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 187, "seek": 64312, "start": 653.36, "end": 654.2, "text": " Is that fair?", "tokens": [50876, 1119, 300, 3143, 30, 50918], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 188, "seek": 64312, "start": 654.2, "end": 655.5600000000001, "text": " And is that a good strategy?", "tokens": [50918, 400, 307, 300, 257, 665, 5206, 30, 50986], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 189, "seek": 64312, "start": 656.24, "end": 657.8, "text": " Uh, right.", "tokens": [51020, 4019, 11, 558, 13, 51098], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 190, "seek": 64312, "start": 657.8, "end": 659.84, "text": " I think it's really, it really depends.", "tokens": [51098, 286, 519, 309, 311, 534, 11, 309, 534, 5946, 13, 51200], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 191, "seek": 64312, "start": 659.84, "end": 665.68, "text": " So my personal experience is that if we are really GPU or computing power constraint,", "tokens": [51200, 407, 452, 2973, 1752, 307, 300, 498, 321, 366, 534, 18407, 420, 15866, 1347, 25534, 11, 51492], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 192, "seek": 64312, "start": 666.2, "end": 671.8, "text": " say if we can't even train the 7 billion model, so LoRa is a perfect technology to", "tokens": [51518, 584, 498, 321, 393, 380, 754, 3847, 264, 1614, 5218, 2316, 11, 370, 6130, 41873, 307, 257, 2176, 2899, 281, 51798], "temperature": 0.0, "avg_logprob": -0.197887942322299, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.004606116097420454}, {"id": 193, "seek": 67180, "start": 671.8, "end": 674.7199999999999, "text": " democratize the model to a whole bunch of people.", "tokens": [50364, 37221, 1125, 264, 2316, 281, 257, 1379, 3840, 295, 561, 13, 50510], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 194, "seek": 67180, "start": 674.7199999999999, "end": 674.92, "text": " Right.", "tokens": [50510, 1779, 13, 50520], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 195, "seek": 67180, "start": 674.92, "end": 679.88, "text": " So currently I think LoRa, the LoRa like software stack and also it's a variant.", "tokens": [50520, 407, 4362, 286, 519, 6130, 41873, 11, 264, 6130, 41873, 411, 4722, 8630, 293, 611, 309, 311, 257, 17501, 13, 50768], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 196, "seek": 67180, "start": 679.88, "end": 684.76, "text": " Let's say the QLoRa is optimized really well to basically just run our MacBook.", "tokens": [50768, 961, 311, 584, 264, 1249, 31645, 41873, 307, 26941, 534, 731, 281, 1936, 445, 1190, 527, 31737, 13, 51012], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 197, "seek": 67180, "start": 685.0, "end": 686.04, "text": " So that's, that's perfect.", "tokens": [51024, 407, 300, 311, 11, 300, 311, 2176, 13, 51076], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 198, "seek": 67180, "start": 686.04, "end": 690.04, "text": " I think a lot of like amazing application has been built on top of that to", "tokens": [51076, 286, 519, 257, 688, 295, 411, 2243, 3861, 575, 668, 3094, 322, 1192, 295, 300, 281, 51276], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 199, "seek": 67180, "start": 690.04, "end": 694.16, "text": " generate their own images, things like that, stable diffusion or some more", "tokens": [51276, 8460, 641, 1065, 5267, 11, 721, 411, 300, 11, 8351, 25242, 420, 512, 544, 51482], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 200, "seek": 67180, "start": 694.16, "end": 696.88, "text": " serious, let's say accuracy driven application.", "tokens": [51482, 3156, 11, 718, 311, 584, 14170, 9555, 3861, 13, 51618], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 201, "seek": 67180, "start": 696.88, "end": 701.4399999999999, "text": " Let's say I myself is building a medical chat board right now and I'm just doing", "tokens": [51618, 961, 311, 584, 286, 2059, 307, 2390, 257, 4625, 5081, 3150, 558, 586, 293, 286, 478, 445, 884, 51846], "temperature": 0.0, "avg_logprob": -0.19473859843085795, "compression_ratio": 1.6762820512820513, "no_speech_prob": 0.0028881309553980827}, {"id": 202, "seek": 70144, "start": 701.44, "end": 705.32, "text": " some like a pre-training plus fine tuning on some medical publications.", "tokens": [50364, 512, 411, 257, 659, 12, 17227, 1760, 1804, 2489, 15164, 322, 512, 4625, 25618, 13, 50558], "temperature": 0.0, "avg_logprob": -0.15637571443387163, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0005111463833600283}, {"id": 203, "seek": 70144, "start": 705.32, "end": 707.6, "text": " Let's say there's a kind of like something called PubMed.", "tokens": [50558, 961, 311, 584, 456, 311, 257, 733, 295, 411, 746, 1219, 21808, 42954, 13, 50672], "temperature": 0.0, "avg_logprob": -0.15637571443387163, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0005111463833600283}, {"id": 204, "seek": 70144, "start": 707.8800000000001, "end": 712.08, "text": " There, I think if we have enough or kind of like decent amount of computing", "tokens": [50686, 821, 11, 286, 519, 498, 321, 362, 1547, 420, 733, 295, 411, 8681, 2372, 295, 15866, 50896], "temperature": 0.0, "avg_logprob": -0.15637571443387163, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0005111463833600283}, {"id": 205, "seek": 70144, "start": 712.08, "end": 716.44, "text": " resources, LoRa seems to lead to some accuracy drops there.", "tokens": [50896, 3593, 11, 6130, 41873, 2544, 281, 1477, 281, 512, 14170, 11438, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15637571443387163, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0005111463833600283}, {"id": 206, "seek": 70144, "start": 716.48, "end": 721.0400000000001, "text": " So let's say if we, if you tune a 65 billion model using LoRa, it", "tokens": [51116, 407, 718, 311, 584, 498, 321, 11, 498, 291, 10864, 257, 11624, 5218, 2316, 1228, 6130, 41873, 11, 309, 51344], "temperature": 0.0, "avg_logprob": -0.15637571443387163, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0005111463833600283}, {"id": 207, "seek": 70144, "start": 721.0400000000001, "end": 725.1600000000001, "text": " won't perform as good as we find for fine tune a 30 billion model.", "tokens": [51344, 1582, 380, 2042, 382, 665, 382, 321, 915, 337, 2489, 10864, 257, 2217, 5218, 2316, 13, 51550], "temperature": 0.0, "avg_logprob": -0.15637571443387163, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0005111463833600283}, {"id": 208, "seek": 70144, "start": 725.36, "end": 728.2800000000001, "text": " So I think it's really depends on applications, but I personally think", "tokens": [51560, 407, 286, 519, 309, 311, 534, 5946, 322, 5821, 11, 457, 286, 5665, 519, 51706], "temperature": 0.0, "avg_logprob": -0.15637571443387163, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0005111463833600283}, {"id": 209, "seek": 72828, "start": 728.3199999999999, "end": 732.9599999999999, "text": " LoRa is a very amazing work for democratizing the powerfulness of the", "tokens": [50366, 6130, 41873, 307, 257, 588, 2243, 589, 337, 37221, 3319, 264, 4005, 1287, 295, 264, 50598], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 210, "seek": 72828, "start": 732.9599999999999, "end": 735.72, "text": " large language model to a wide amount of users.", "tokens": [50598, 2416, 2856, 2316, 281, 257, 4874, 2372, 295, 5022, 13, 50736], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 211, "seek": 72828, "start": 736.0799999999999, "end": 740.28, "text": " Well, how would you contrast LoRa to Cuttlefish and maybe also can you at", "tokens": [50754, 1042, 11, 577, 576, 291, 8712, 6130, 41873, 281, 9431, 10972, 11608, 293, 1310, 611, 393, 291, 412, 50964], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 212, "seek": 72828, "start": 740.28, "end": 741.92, "text": " the same time introduce Cuttlefish?", "tokens": [50964, 264, 912, 565, 5366, 9431, 10972, 11608, 30, 51046], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 213, "seek": 72828, "start": 741.9599999999999, "end": 742.8399999999999, "text": " Oh yeah, definitely.", "tokens": [51048, 876, 1338, 11, 2138, 13, 51092], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 214, "seek": 72828, "start": 742.8399999999999, "end": 747.72, "text": " So Cuttlefish is actually not even the first attempt for this line of work.", "tokens": [51092, 407, 9431, 10972, 11608, 307, 767, 406, 754, 264, 700, 5217, 337, 341, 1622, 295, 589, 13, 51336], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 215, "seek": 72828, "start": 747.88, "end": 752.68, "text": " Discuss with the LoRa team, we had had a few official meetings actually.", "tokens": [51344, 4208, 2169, 365, 264, 6130, 41873, 1469, 11, 321, 632, 632, 257, 1326, 4783, 8410, 767, 13, 51584], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 216, "seek": 72828, "start": 753.0799999999999, "end": 756.1999999999999, "text": " So Cuttlefish is something like, so we're just treating the model,", "tokens": [51604, 407, 9431, 10972, 11608, 307, 746, 411, 11, 370, 321, 434, 445, 15083, 264, 2316, 11, 51760], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 217, "seek": 72828, "start": 756.1999999999999, "end": 757.92, "text": " it's a model is just too huge.", "tokens": [51760, 309, 311, 257, 2316, 307, 445, 886, 2603, 13, 51846], "temperature": 0.0, "avg_logprob": -0.22024123580367477, "compression_ratio": 1.683673469387755, "no_speech_prob": 0.07154937833547592}, {"id": 218, "seek": 75792, "start": 758.12, "end": 761.0799999999999, "text": " So as a beginning, we are thinking about how can we make kind of like", "tokens": [50374, 407, 382, 257, 2863, 11, 321, 366, 1953, 466, 577, 393, 321, 652, 733, 295, 411, 50522], "temperature": 0.0, "avg_logprob": -0.20219414809654498, "compression_ratio": 2.008298755186722, "no_speech_prob": 0.0004954267642460763}, {"id": 219, "seek": 75792, "start": 761.0799999999999, "end": 762.4799999999999, "text": " mixing a little bit easier.", "tokens": [50522, 11983, 257, 707, 857, 3571, 13, 50592], "temperature": 0.0, "avg_logprob": -0.20219414809654498, "compression_ratio": 2.008298755186722, "no_speech_prob": 0.0004954267642460763}, {"id": 220, "seek": 75792, "start": 762.68, "end": 765.1999999999999, "text": " Turns out we can just treat a low rank model.", "tokens": [50602, 29524, 484, 321, 393, 445, 2387, 257, 2295, 6181, 2316, 13, 50728], "temperature": 0.0, "avg_logprob": -0.20219414809654498, "compression_ratio": 2.008298755186722, "no_speech_prob": 0.0004954267642460763}, {"id": 221, "seek": 75792, "start": 765.5999999999999, "end": 768.76, "text": " So low rank is basically, it's a kind of like a basic linear algebra.", "tokens": [50748, 407, 2295, 6181, 307, 1936, 11, 309, 311, 257, 733, 295, 411, 257, 3875, 8213, 21989, 13, 50906], "temperature": 0.0, "avg_logprob": -0.20219414809654498, "compression_ratio": 2.008298755186722, "no_speech_prob": 0.0004954267642460763}, {"id": 222, "seek": 75792, "start": 768.76, "end": 772.4799999999999, "text": " So our model is basically a whole bunch of, we can understand that the whole", "tokens": [50906, 407, 527, 2316, 307, 1936, 257, 1379, 3840, 295, 11, 321, 393, 1223, 300, 264, 1379, 51092], "temperature": 0.0, "avg_logprob": -0.20219414809654498, "compression_ratio": 2.008298755186722, "no_speech_prob": 0.0004954267642460763}, {"id": 223, "seek": 75792, "start": 772.4799999999999, "end": 776.7199999999999, "text": " bunch of matrix concatenating together and we are kind of like computing one", "tokens": [51092, 3840, 295, 8141, 1588, 7186, 990, 1214, 293, 321, 366, 733, 295, 411, 15866, 472, 51304], "temperature": 0.0, "avg_logprob": -0.20219414809654498, "compression_ratio": 2.008298755186722, "no_speech_prob": 0.0004954267642460763}, {"id": 224, "seek": 75792, "start": 776.7199999999999, "end": 779.64, "text": " matrix multiplication by one matrix multiplication.", "tokens": [51304, 8141, 27290, 538, 472, 8141, 27290, 13, 51450], "temperature": 0.0, "avg_logprob": -0.20219414809654498, "compression_ratio": 2.008298755186722, "no_speech_prob": 0.0004954267642460763}, {"id": 225, "seek": 75792, "start": 780.12, "end": 783.8399999999999, "text": " So low rank approximation is basically we approximate the matrix", "tokens": [51474, 407, 2295, 6181, 28023, 307, 1936, 321, 30874, 264, 8141, 51660], "temperature": 0.0, "avg_logprob": -0.20219414809654498, "compression_ratio": 2.008298755186722, "no_speech_prob": 0.0004954267642460763}, {"id": 226, "seek": 78384, "start": 783.88, "end": 788.24, "text": " multiplication using fewer number of flops.", "tokens": [50366, 27290, 1228, 13366, 1230, 295, 932, 3370, 13, 50584], "temperature": 0.0, "avg_logprob": -0.20881088574727377, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.051800042390823364}, {"id": 227, "seek": 78384, "start": 788.36, "end": 792.6800000000001, "text": " And also we can also make the matrix a little bit smaller, such that the", "tokens": [50590, 400, 611, 321, 393, 611, 652, 264, 8141, 257, 707, 857, 4356, 11, 1270, 300, 264, 50806], "temperature": 0.0, "avg_logprob": -0.20881088574727377, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.051800042390823364}, {"id": 228, "seek": 78384, "start": 792.6800000000001, "end": 795.4, "text": " computation can be a bit faster.", "tokens": [50806, 24903, 393, 312, 257, 857, 4663, 13, 50942], "temperature": 0.0, "avg_logprob": -0.20881088574727377, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.051800042390823364}, {"id": 229, "seek": 78384, "start": 795.8000000000001, "end": 799.52, "text": " We also save a little bit of GPU memory and also communication become faster", "tokens": [50962, 492, 611, 3155, 257, 707, 857, 295, 18407, 4675, 293, 611, 6101, 1813, 4663, 51148], "temperature": 0.0, "avg_logprob": -0.20881088574727377, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.051800042390823364}, {"id": 230, "seek": 78384, "start": 799.52, "end": 801.8000000000001, "text": " because we are essentially training a smaller model.", "tokens": [51148, 570, 321, 366, 4476, 3097, 257, 4356, 2316, 13, 51262], "temperature": 0.0, "avg_logprob": -0.20881088574727377, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.051800042390823364}, {"id": 231, "seek": 78384, "start": 802.2, "end": 806.24, "text": " That's basically the basic technology behind Cuttlefish.", "tokens": [51282, 663, 311, 1936, 264, 3875, 2899, 2261, 9431, 10972, 11608, 13, 51484], "temperature": 0.0, "avg_logprob": -0.20881088574727377, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.051800042390823364}, {"id": 232, "seek": 78384, "start": 806.64, "end": 811.08, "text": " And actually we, at the beginning, we published a paper in 2021 called", "tokens": [51504, 400, 767, 321, 11, 412, 264, 2863, 11, 321, 6572, 257, 3035, 294, 7201, 1219, 51726], "temperature": 0.0, "avg_logprob": -0.20881088574727377, "compression_ratio": 1.6345381526104417, "no_speech_prob": 0.051800042390823364}, {"id": 233, "seek": 81108, "start": 811.12, "end": 813.9200000000001, "text": " Pufferfish. So that's the very initial work.", "tokens": [50366, 430, 1245, 260, 11608, 13, 407, 300, 311, 264, 588, 5883, 589, 13, 50506], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 234, "seek": 81108, "start": 813.9200000000001, "end": 816.84, "text": " It's just doing this approximation and the training from scratch.", "tokens": [50506, 467, 311, 445, 884, 341, 28023, 293, 264, 3097, 490, 8459, 13, 50652], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 235, "seek": 81108, "start": 817.12, "end": 820.76, "text": " And turns out when you train the low rank model, it doesn't work as well as", "tokens": [50666, 400, 4523, 484, 562, 291, 3847, 264, 2295, 6181, 2316, 11, 309, 1177, 380, 589, 382, 731, 382, 50848], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 236, "seek": 81108, "start": 820.76, "end": 824.36, "text": " a full rank model because we kind of like to reduce a lot of model capacity.", "tokens": [50848, 257, 1577, 6181, 2316, 570, 321, 733, 295, 411, 281, 5407, 257, 688, 295, 2316, 6042, 13, 51028], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 237, "seek": 81108, "start": 824.5600000000001, "end": 825.8000000000001, "text": " It doesn't generalize well.", "tokens": [51038, 467, 1177, 380, 2674, 1125, 731, 13, 51100], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 238, "seek": 81108, "start": 826.0400000000001, "end": 828.9200000000001, "text": " Then we kind of like introduced several technologies there.", "tokens": [51112, 1396, 321, 733, 295, 411, 7268, 2940, 7943, 456, 13, 51256], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 239, "seek": 81108, "start": 829.08, "end": 832.2800000000001, "text": " Some of the layer it's hard to approximate, right?", "tokens": [51264, 2188, 295, 264, 4583, 309, 311, 1152, 281, 30874, 11, 558, 30, 51424], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 240, "seek": 81108, "start": 832.2800000000001, "end": 836.72, "text": " If you kind of like approximate that it will cause a huge amount of accuracy", "tokens": [51424, 759, 291, 733, 295, 411, 30874, 300, 309, 486, 3082, 257, 2603, 2372, 295, 14170, 51646], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 241, "seek": 81108, "start": 836.72, "end": 838.96, "text": " drop. For some of the layer it's fine.", "tokens": [51646, 3270, 13, 1171, 512, 295, 264, 4583, 309, 311, 2489, 13, 51758], "temperature": 0.0, "avg_logprob": -0.21159061855740016, "compression_ratio": 1.7862068965517242, "no_speech_prob": 0.07358357310295105}, {"id": 242, "seek": 83896, "start": 838.96, "end": 840.2800000000001, "text": " So that's the first technology.", "tokens": [50364, 407, 300, 311, 264, 700, 2899, 13, 50430], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 243, "seek": 83896, "start": 840.2800000000001, "end": 843.88, "text": " We're just selectively factorizing some of the layers in the neural network.", "tokens": [50430, 492, 434, 445, 3048, 3413, 5952, 3319, 512, 295, 264, 7914, 294, 264, 18161, 3209, 13, 50610], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 244, "seek": 83896, "start": 844.2800000000001, "end": 847.96, "text": " Second technology is that you may not want to go low rank in the very beginning.", "tokens": [50630, 5736, 2899, 307, 300, 291, 815, 406, 528, 281, 352, 2295, 6181, 294, 264, 588, 2863, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 245, "seek": 83896, "start": 848.24, "end": 850.1600000000001, "text": " So it's mostly like the sparse training, right?", "tokens": [50828, 407, 309, 311, 5240, 411, 264, 637, 11668, 3097, 11, 558, 30, 50924], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 246, "seek": 83896, "start": 850.1600000000001, "end": 853.64, "text": " So in the beginning you want to do this and you gradually make your model become", "tokens": [50924, 407, 294, 264, 2863, 291, 528, 281, 360, 341, 293, 291, 13145, 652, 428, 2316, 1813, 51098], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 247, "seek": 83896, "start": 853.64, "end": 857.2, "text": " sparse. In the full rank to low rank training, we also doing things like that.", "tokens": [51098, 637, 11668, 13, 682, 264, 1577, 6181, 281, 2295, 6181, 3097, 11, 321, 611, 884, 721, 411, 300, 13, 51276], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 248, "seek": 83896, "start": 857.2, "end": 861.44, "text": " In the beginning, we are doing full rank model and then we gradually start to", "tokens": [51276, 682, 264, 2863, 11, 321, 366, 884, 1577, 6181, 2316, 293, 550, 321, 13145, 722, 281, 51488], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 249, "seek": 83896, "start": 861.6800000000001, "end": 863.6800000000001, "text": " convert from full rank to low rank.", "tokens": [51500, 7620, 490, 1577, 6181, 281, 2295, 6181, 13, 51600], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 250, "seek": 83896, "start": 863.84, "end": 866.52, "text": " So that's called Pufferfish published in 2021.", "tokens": [51608, 407, 300, 311, 1219, 430, 1245, 260, 11608, 6572, 294, 7201, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1671483366639464, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.0013452970888465643}, {"id": 251, "seek": 86652, "start": 866.84, "end": 869.1999999999999, "text": " Pufferfish took that a step further.", "tokens": [50380, 430, 1245, 260, 11608, 1890, 300, 257, 1823, 3052, 13, 50498], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 252, "seek": 86652, "start": 869.1999999999999, "end": 873.76, "text": " It's basically so when we switch from full rank to low rank and which layers we", "tokens": [50498, 467, 311, 1936, 370, 562, 321, 3679, 490, 1577, 6181, 281, 2295, 6181, 293, 597, 7914, 321, 50726], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 253, "seek": 86652, "start": 873.76, "end": 874.72, "text": " don't want to factorize.", "tokens": [50726, 500, 380, 528, 281, 5952, 1125, 13, 50774], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 254, "seek": 86652, "start": 874.72, "end": 877.56, "text": " So those are a lot of like heavy hyperparameter tuning.", "tokens": [50774, 407, 729, 366, 257, 688, 295, 411, 4676, 9848, 2181, 335, 2398, 15164, 13, 50916], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 255, "seek": 86652, "start": 877.72, "end": 879.88, "text": " And for each of the layer, how much should we probe?", "tokens": [50924, 400, 337, 1184, 295, 264, 4583, 11, 577, 709, 820, 321, 22715, 30, 51032], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 256, "seek": 86652, "start": 880.1999999999999, "end": 884.04, "text": " Right. That's also a kind of like a fundamental problem faced in the sparse", "tokens": [51048, 1779, 13, 663, 311, 611, 257, 733, 295, 411, 257, 8088, 1154, 11446, 294, 264, 637, 11668, 51240], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 257, "seek": 86652, "start": 884.04, "end": 885.16, "text": " training community.", "tokens": [51240, 3097, 1768, 13, 51296], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 258, "seek": 86652, "start": 885.3199999999999, "end": 888.96, "text": " For each of the layer, you may not want to sparsify them in the same ratio.", "tokens": [51304, 1171, 1184, 295, 264, 4583, 11, 291, 815, 406, 528, 281, 637, 685, 2505, 552, 294, 264, 912, 8509, 13, 51486], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 259, "seek": 86652, "start": 889.24, "end": 893.52, "text": " Pufferfish take everything automatic and we kind of like make the training.", "tokens": [51500, 430, 1245, 260, 11608, 747, 1203, 12509, 293, 321, 733, 295, 411, 652, 264, 3097, 13, 51714], "temperature": 0.0, "avg_logprob": -0.21631258943655196, "compression_ratio": 1.711340206185567, "no_speech_prob": 0.008310913108289242}, {"id": 260, "seek": 89352, "start": 893.56, "end": 898.1999999999999, "text": " During the training, we detect some of the heuristics and we try to make everything", "tokens": [50366, 6842, 264, 3097, 11, 321, 5531, 512, 295, 264, 415, 374, 6006, 293, 321, 853, 281, 652, 1203, 50598], "temperature": 0.0, "avg_logprob": -0.16084902165299755, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.004329176153987646}, {"id": 261, "seek": 89352, "start": 898.1999999999999, "end": 900.84, "text": " automatic. So basically select each of the layers.", "tokens": [50598, 12509, 13, 407, 1936, 3048, 1184, 295, 264, 7914, 13, 50730], "temperature": 0.0, "avg_logprob": -0.16084902165299755, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.004329176153987646}, {"id": 262, "seek": 89352, "start": 900.84, "end": 906.0, "text": " Select only the layer that's tolerable for low rank factorization and", "tokens": [50730, 13638, 787, 264, 4583, 300, 311, 11125, 712, 337, 2295, 6181, 5952, 2144, 293, 50988], "temperature": 0.0, "avg_logprob": -0.16084902165299755, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.004329176153987646}, {"id": 263, "seek": 89352, "start": 906.0, "end": 909.4399999999999, "text": " automatically detect when should we switch from full rank to low rank.", "tokens": [50988, 6772, 5531, 562, 820, 321, 3679, 490, 1577, 6181, 281, 2295, 6181, 13, 51160], "temperature": 0.0, "avg_logprob": -0.16084902165299755, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.004329176153987646}, {"id": 264, "seek": 89352, "start": 910.12, "end": 912.88, "text": " Basically make everything more hyperparameter free.", "tokens": [51194, 8537, 652, 1203, 544, 9848, 2181, 335, 2398, 1737, 13, 51332], "temperature": 0.0, "avg_logprob": -0.16084902165299755, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.004329176153987646}, {"id": 265, "seek": 89352, "start": 913.04, "end": 916.8, "text": " It's not totally hyperparameter free, but something like more automatic.", "tokens": [51340, 467, 311, 406, 3879, 9848, 2181, 335, 2398, 1737, 11, 457, 746, 411, 544, 12509, 13, 51528], "temperature": 0.0, "avg_logprob": -0.16084902165299755, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.004329176153987646}, {"id": 266, "seek": 89352, "start": 917.1999999999999, "end": 921.88, "text": " Well, if we're to maybe frame it in terms of tradeoffs, if I want to take advantage", "tokens": [51548, 1042, 11, 498, 321, 434, 281, 1310, 3920, 309, 294, 2115, 295, 4923, 19231, 11, 498, 286, 528, 281, 747, 5002, 51782], "temperature": 0.0, "avg_logprob": -0.16084902165299755, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.004329176153987646}, {"id": 267, "seek": 92188, "start": 921.88, "end": 924.52, "text": " of these efficiencies, do I have to give something up?", "tokens": [50364, 295, 613, 4703, 31294, 11, 360, 286, 362, 281, 976, 746, 493, 30, 50496], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 268, "seek": 92188, "start": 924.72, "end": 925.56, "text": " And if so, what?", "tokens": [50506, 400, 498, 370, 11, 437, 30, 50548], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 269, "seek": 92188, "start": 926.0, "end": 930.4, "text": " Most of the critical part is that we have to give up some of the accuracy if we", "tokens": [50570, 4534, 295, 264, 4924, 644, 307, 300, 321, 362, 281, 976, 493, 512, 295, 264, 14170, 498, 321, 50790], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 270, "seek": 92188, "start": 930.4, "end": 931.56, "text": " don't do it quite well.", "tokens": [50790, 500, 380, 360, 309, 1596, 731, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 271, "seek": 92188, "start": 932.12, "end": 936.96, "text": " The entire work is basically depend on how much the redundancy is in the model.", "tokens": [50876, 440, 2302, 589, 307, 1936, 5672, 322, 577, 709, 264, 27830, 6717, 307, 294, 264, 2316, 13, 51118], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 272, "seek": 92188, "start": 937.16, "end": 941.56, "text": " So if the model parameter does not contain redundancy at all, this method will fail.", "tokens": [51128, 407, 498, 264, 2316, 13075, 775, 406, 5304, 27830, 6717, 412, 439, 11, 341, 3170, 486, 3061, 13, 51348], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 273, "seek": 92188, "start": 941.6, "end": 943.2, "text": " So I have to be very honest.", "tokens": [51350, 407, 286, 362, 281, 312, 588, 3245, 13, 51430], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 274, "seek": 92188, "start": 943.68, "end": 946.76, "text": " It's about how to detect the model's parameters redundancy.", "tokens": [51454, 467, 311, 466, 577, 281, 5531, 264, 2316, 311, 9834, 27830, 6717, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 275, "seek": 92188, "start": 946.76, "end": 949.08, "text": " If there's no redundancy, don't use this.", "tokens": [51608, 759, 456, 311, 572, 27830, 6717, 11, 500, 380, 764, 341, 13, 51724], "temperature": 0.0, "avg_logprob": -0.1512077793930516, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.008312131278216839}, {"id": 276, "seek": 94908, "start": 949.36, "end": 953.32, "text": " And if there's enough redundancy, then we should detect what's the level of the", "tokens": [50378, 400, 498, 456, 311, 1547, 27830, 6717, 11, 550, 321, 820, 5531, 437, 311, 264, 1496, 295, 264, 50576], "temperature": 0.0, "avg_logprob": -0.22062434669302292, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.01797274872660637}, {"id": 277, "seek": 94908, "start": 953.32, "end": 957.12, "text": " redundancy and we should remove the exact amount of the redundancy.", "tokens": [50576, 27830, 6717, 293, 321, 820, 4159, 264, 1900, 2372, 295, 264, 27830, 6717, 13, 50766], "temperature": 0.0, "avg_logprob": -0.22062434669302292, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.01797274872660637}, {"id": 278, "seek": 94908, "start": 957.5200000000001, "end": 962.0, "text": " So in the PowerFish paper, we did that in a very heuristic way.", "tokens": [50786, 407, 294, 264, 7086, 37, 742, 3035, 11, 321, 630, 300, 294, 257, 588, 415, 374, 3142, 636, 13, 51010], "temperature": 0.0, "avg_logprob": -0.22062434669302292, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.01797274872660637}, {"id": 279, "seek": 94908, "start": 962.0, "end": 963.12, "text": " Sometimes it won't work.", "tokens": [51010, 4803, 309, 1582, 380, 589, 13, 51066], "temperature": 0.0, "avg_logprob": -0.22062434669302292, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.01797274872660637}, {"id": 280, "seek": 94908, "start": 963.12, "end": 966.2800000000001, "text": " You can like, as you mentioned, we will sacrifice some accuracy.", "tokens": [51066, 509, 393, 411, 11, 382, 291, 2835, 11, 321, 486, 11521, 512, 14170, 13, 51224], "temperature": 0.0, "avg_logprob": -0.22062434669302292, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.01797274872660637}, {"id": 281, "seek": 94908, "start": 966.48, "end": 971.44, "text": " In Cuttlefish, we are trying to optimize that really well to automatically detect", "tokens": [51234, 682, 9431, 10972, 11608, 11, 321, 366, 1382, 281, 19719, 300, 534, 731, 281, 6772, 5531, 51482], "temperature": 0.0, "avg_logprob": -0.22062434669302292, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.01797274872660637}, {"id": 282, "seek": 94908, "start": 971.44, "end": 976.5600000000001, "text": " the redundancy level and then we just remove those appropriate amount of redundancy.", "tokens": [51482, 264, 27830, 6717, 1496, 293, 550, 321, 445, 4159, 729, 6854, 2372, 295, 27830, 6717, 13, 51738], "temperature": 0.0, "avg_logprob": -0.22062434669302292, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.01797274872660637}, {"id": 283, "seek": 97656, "start": 977.04, "end": 980.7199999999999, "text": " And it turns out it kind of like mitigates accuracy drop very well.", "tokens": [50388, 400, 309, 4523, 484, 309, 733, 295, 411, 15699, 1024, 14170, 3270, 588, 731, 13, 50572], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 284, "seek": 97656, "start": 981.1999999999999, "end": 982.4799999999999, "text": " I appreciate your point.", "tokens": [50596, 286, 4449, 428, 935, 13, 50660], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 285, "seek": 97656, "start": 982.4799999999999, "end": 986.8399999999999, "text": " If there's no redundancy, there's nothing to, there's no advantage to be gained.", "tokens": [50660, 759, 456, 311, 572, 27830, 6717, 11, 456, 311, 1825, 281, 11, 456, 311, 572, 5002, 281, 312, 12634, 13, 50878], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 286, "seek": 97656, "start": 987.0799999999999, "end": 987.28, "text": " Yeah.", "tokens": [50890, 865, 13, 50900], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 287, "seek": 97656, "start": 987.28, "end": 991.4799999999999, "text": " But when I hear about these models that have, you know, increasingly billions of", "tokens": [50900, 583, 562, 286, 1568, 466, 613, 5245, 300, 362, 11, 291, 458, 11, 12980, 17375, 295, 51110], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 288, "seek": 97656, "start": 991.4799999999999, "end": 994.88, "text": " parameters, it's hard to believe there isn't redundancy here.", "tokens": [51110, 9834, 11, 309, 311, 1152, 281, 1697, 456, 1943, 380, 27830, 6717, 510, 13, 51280], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 289, "seek": 97656, "start": 994.9599999999999, "end": 996.16, "text": " Is that a fair intuition?", "tokens": [51284, 1119, 300, 257, 3143, 24002, 30, 51344], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 290, "seek": 97656, "start": 996.92, "end": 997.92, "text": " I think so.", "tokens": [51382, 286, 519, 370, 13, 51432], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 291, "seek": 97656, "start": 997.92, "end": 1001.5999999999999, "text": " So I think the Scouting Law paper is a very, very good one.", "tokens": [51432, 407, 286, 519, 264, 2747, 24500, 7744, 3035, 307, 257, 588, 11, 588, 665, 472, 13, 51616], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 292, "seek": 97656, "start": 1001.64, "end": 1003.76, "text": " I actually encourage people to read.", "tokens": [51618, 286, 767, 5373, 561, 281, 1401, 13, 51724], "temperature": 0.0, "avg_logprob": -0.17278291296771192, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.01853027381002903}, {"id": 293, "seek": 100376, "start": 1004.0, "end": 1008.64, "text": " So basically that's basically telling us that the data should scale proportionately", "tokens": [50376, 407, 1936, 300, 311, 1936, 3585, 505, 300, 264, 1412, 820, 4373, 16068, 1592, 50608], "temperature": 0.0, "avg_logprob": -0.20522282266209269, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.003592226654291153}, {"id": 294, "seek": 100376, "start": 1008.64, "end": 1009.72, "text": " to the model size, right?", "tokens": [50608, 281, 264, 2316, 2744, 11, 558, 30, 50662], "temperature": 0.0, "avg_logprob": -0.20522282266209269, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.003592226654291153}, {"id": 295, "seek": 100376, "start": 1009.72, "end": 1016.12, "text": " So otherwise, if we just scale the model up, let's say to one training or whatever,", "tokens": [50662, 407, 5911, 11, 498, 321, 445, 4373, 264, 2316, 493, 11, 718, 311, 584, 281, 472, 3097, 420, 2035, 11, 50982], "temperature": 0.0, "avg_logprob": -0.20522282266209269, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.003592226654291153}, {"id": 296, "seek": 100376, "start": 1016.28, "end": 1020.24, "text": " it will only increase the redundancy in the model parameters.", "tokens": [50990, 309, 486, 787, 3488, 264, 27830, 6717, 294, 264, 2316, 9834, 13, 51188], "temperature": 0.0, "avg_logprob": -0.20522282266209269, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.003592226654291153}, {"id": 297, "seek": 100376, "start": 1020.52, "end": 1024.6, "text": " That will make PowerFish work better or Cuttlefish work better, but that's not", "tokens": [51202, 663, 486, 652, 7086, 37, 742, 589, 1101, 420, 9431, 10972, 11608, 589, 1101, 11, 457, 300, 311, 406, 51406], "temperature": 0.0, "avg_logprob": -0.20522282266209269, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.003592226654291153}, {"id": 298, "seek": 100376, "start": 1024.6, "end": 1027.2, "text": " what we want to see actually in the application.", "tokens": [51406, 437, 321, 528, 281, 536, 767, 294, 264, 3861, 13, 51536], "temperature": 0.0, "avg_logprob": -0.20522282266209269, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.003592226654291153}, {"id": 299, "seek": 100376, "start": 1027.44, "end": 1029.72, "text": " Like we should really control that very well.", "tokens": [51548, 1743, 321, 820, 534, 1969, 300, 588, 731, 13, 51662], "temperature": 0.0, "avg_logprob": -0.20522282266209269, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.003592226654291153}, {"id": 300, "seek": 100376, "start": 1030.12, "end": 1032.2, "text": " I guess that's basically my intuition.", "tokens": [51682, 286, 2041, 300, 311, 1936, 452, 24002, 13, 51786], "temperature": 0.0, "avg_logprob": -0.20522282266209269, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.003592226654291153}, {"id": 301, "seek": 103220, "start": 1032.52, "end": 1035.8400000000001, "text": " There's no kind of like a qualitative way to do that yet.", "tokens": [50380, 821, 311, 572, 733, 295, 411, 257, 31312, 636, 281, 360, 300, 1939, 13, 50546], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 302, "seek": 103220, "start": 1035.92, "end": 1038.96, "text": " I already encourage people to look into that research direction.", "tokens": [50550, 286, 1217, 5373, 561, 281, 574, 666, 300, 2132, 3513, 13, 50702], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 303, "seek": 103220, "start": 1039.6000000000001, "end": 1043.3600000000001, "text": " Well, when I think about the people training large language models, in my", "tokens": [50734, 1042, 11, 562, 286, 519, 466, 264, 561, 3097, 2416, 2856, 5245, 11, 294, 452, 50922], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 304, "seek": 103220, "start": 1043.3600000000001, "end": 1044.76, "text": " head, there's two categories.", "tokens": [50922, 1378, 11, 456, 311, 732, 10479, 13, 50992], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 305, "seek": 103220, "start": 1044.76, "end": 1049.0800000000002, "text": " There's people like the ones at OpenAI who are working on the foundational models.", "tokens": [50992, 821, 311, 561, 411, 264, 2306, 412, 7238, 48698, 567, 366, 1364, 322, 264, 32195, 5245, 13, 51208], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 306, "seek": 103220, "start": 1049.1200000000001, "end": 1049.44, "text": " Yeah.", "tokens": [51210, 865, 13, 51226], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 307, "seek": 103220, "start": 1049.44, "end": 1053.1200000000001, "text": " Maybe training the next DaVinci or ChatGPT or whatever it is.", "tokens": [51226, 2704, 3097, 264, 958, 3933, 53, 21961, 420, 27503, 38, 47, 51, 420, 2035, 309, 307, 13, 51410], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 308, "seek": 103220, "start": 1053.72, "end": 1056.68, "text": " And then there are those who want to extend it to their own domain.", "tokens": [51440, 400, 550, 456, 366, 729, 567, 528, 281, 10101, 309, 281, 641, 1065, 9274, 13, 51588], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 309, "seek": 103220, "start": 1056.8, "end": 1057.2, "text": " Right.", "tokens": [51594, 1779, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 310, "seek": 103220, "start": 1057.44, "end": 1060.64, "text": " Who should, which or both, who should be using Cuttlefish?", "tokens": [51626, 2102, 820, 11, 597, 420, 1293, 11, 567, 820, 312, 1228, 9431, 10972, 11608, 30, 51786], "temperature": 0.0, "avg_logprob": -0.19025280527824903, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.004901166073977947}, {"id": 311, "seek": 106064, "start": 1061.2800000000002, "end": 1067.3600000000001, "text": " Well, I really want to say both, but for the first principle, Cuttlefish is", "tokens": [50396, 1042, 11, 286, 534, 528, 281, 584, 1293, 11, 457, 337, 264, 700, 8665, 11, 9431, 10972, 11608, 307, 50700], "temperature": 0.0, "avg_logprob": -0.16159845571048925, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.0012838076800107956}, {"id": 312, "seek": 106064, "start": 1067.3600000000001, "end": 1069.2800000000002, "text": " designed for pre-training, I would say.", "tokens": [50700, 4761, 337, 659, 12, 17227, 1760, 11, 286, 576, 584, 13, 50796], "temperature": 0.0, "avg_logprob": -0.16159845571048925, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.0012838076800107956}, {"id": 313, "seek": 106064, "start": 1069.48, "end": 1074.96, "text": " For fine tuning, I think Laura works reasonably well, but it doesn't mean", "tokens": [50806, 1171, 2489, 15164, 11, 286, 519, 13220, 1985, 23551, 731, 11, 457, 309, 1177, 380, 914, 51080], "temperature": 0.0, "avg_logprob": -0.16159845571048925, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.0012838076800107956}, {"id": 314, "seek": 106064, "start": 1074.96, "end": 1078.1200000000001, "text": " like the Cuttlefish cannot support fine tuning.", "tokens": [51080, 411, 264, 9431, 10972, 11608, 2644, 1406, 2489, 15164, 13, 51238], "temperature": 0.0, "avg_logprob": -0.16159845571048925, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.0012838076800107956}, {"id": 315, "seek": 106064, "start": 1078.16, "end": 1081.8400000000001, "text": " It's mostly like you take the model checkpoint, you just decompose them and", "tokens": [51240, 467, 311, 5240, 411, 291, 747, 264, 2316, 42269, 11, 291, 445, 22867, 541, 552, 293, 51424], "temperature": 0.0, "avg_logprob": -0.16159845571048925, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.0012838076800107956}, {"id": 316, "seek": 106064, "start": 1081.8400000000001, "end": 1084.88, "text": " you just keep fine-tune on your own data set.", "tokens": [51424, 291, 445, 1066, 2489, 12, 83, 2613, 322, 428, 1065, 1412, 992, 13, 51576], "temperature": 0.0, "avg_logprob": -0.16159845571048925, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.0012838076800107956}, {"id": 317, "seek": 106064, "start": 1085.3600000000001, "end": 1085.5600000000002, "text": " Yeah.", "tokens": [51600, 865, 13, 51610], "temperature": 0.0, "avg_logprob": -0.16159845571048925, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.0012838076800107956}, {"id": 318, "seek": 106064, "start": 1085.5600000000002, "end": 1088.8000000000002, "text": " I would say mostly for pre-training, at least in the current stage.", "tokens": [51610, 286, 576, 584, 5240, 337, 659, 12, 17227, 1760, 11, 412, 1935, 294, 264, 2190, 3233, 13, 51772], "temperature": 0.0, "avg_logprob": -0.16159845571048925, "compression_ratio": 1.7182539682539681, "no_speech_prob": 0.0012838076800107956}, {"id": 319, "seek": 108880, "start": 1089.24, "end": 1091.9199999999998, "text": " And can you talk a little bit about some empirical results?", "tokens": [50386, 400, 393, 291, 751, 257, 707, 857, 466, 512, 31886, 3542, 30, 50520], "temperature": 0.0, "avg_logprob": -0.26136888306716394, "compression_ratio": 1.5745454545454545, "no_speech_prob": 0.0012841090792790055}, {"id": 320, "seek": 108880, "start": 1091.9199999999998, "end": 1095.04, "text": " How do you measure the improvement gains you get with the methods?", "tokens": [50520, 1012, 360, 291, 3481, 264, 10444, 16823, 291, 483, 365, 264, 7150, 30, 50676], "temperature": 0.0, "avg_logprob": -0.26136888306716394, "compression_ratio": 1.5745454545454545, "no_speech_prob": 0.0012841090792790055}, {"id": 321, "seek": 108880, "start": 1095.8, "end": 1100.2, "text": " First of all, it's a very, the most straightforward one is the model size.", "tokens": [50714, 2386, 295, 439, 11, 309, 311, 257, 588, 11, 264, 881, 15325, 472, 307, 264, 2316, 2744, 13, 50934], "temperature": 0.0, "avg_logprob": -0.26136888306716394, "compression_ratio": 1.5745454545454545, "no_speech_prob": 0.0012841090792790055}, {"id": 322, "seek": 108880, "start": 1100.2, "end": 1100.48, "text": " Right?", "tokens": [50934, 1779, 30, 50948], "temperature": 0.0, "avg_logprob": -0.26136888306716394, "compression_ratio": 1.5745454545454545, "no_speech_prob": 0.0012841090792790055}, {"id": 323, "seek": 108880, "start": 1100.48, "end": 1102.72, "text": " So like when we kind of decompose the model.", "tokens": [50948, 407, 411, 562, 321, 733, 295, 22867, 541, 264, 2316, 13, 51060], "temperature": 0.0, "avg_logprob": -0.26136888306716394, "compression_ratio": 1.5745454545454545, "no_speech_prob": 0.0012841090792790055}, {"id": 324, "seek": 108880, "start": 1103.04, "end": 1106.44, "text": " So currently let's say, actually right now I'm training a 1 billion", "tokens": [51076, 407, 4362, 718, 311, 584, 11, 767, 558, 586, 286, 478, 3097, 257, 502, 5218, 51246], "temperature": 0.0, "avg_logprob": -0.26136888306716394, "compression_ratio": 1.5745454545454545, "no_speech_prob": 0.0012841090792790055}, {"id": 325, "seek": 108880, "start": 1106.48, "end": 1108.8799999999999, "text": " llama to model like using Cuttlefish.", "tokens": [51248, 23272, 281, 2316, 411, 1228, 9431, 10972, 11608, 13, 51368], "temperature": 0.0, "avg_logprob": -0.26136888306716394, "compression_ratio": 1.5745454545454545, "no_speech_prob": 0.0012841090792790055}, {"id": 326, "seek": 108880, "start": 1108.9199999999998, "end": 1115.2, "text": " We can kind of like reach to, we just train something like a 60% smaller,", "tokens": [51370, 492, 393, 733, 295, 411, 2524, 281, 11, 321, 445, 3847, 746, 411, 257, 4060, 4, 4356, 11, 51684], "temperature": 0.0, "avg_logprob": -0.26136888306716394, "compression_ratio": 1.5745454545454545, "no_speech_prob": 0.0012841090792790055}, {"id": 327, "seek": 111520, "start": 1115.24, "end": 1121.16, "text": " to some model with a 60%, you can roughly understand that's a 600 million,", "tokens": [50366, 281, 512, 2316, 365, 257, 4060, 8923, 291, 393, 9810, 1223, 300, 311, 257, 11849, 2459, 11, 50662], "temperature": 0.0, "avg_logprob": -0.24533740234375, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.005908492486923933}, {"id": 328, "seek": 111520, "start": 1121.24, "end": 1124.76, "text": " let's say llama to model subset of the pile data set.", "tokens": [50666, 718, 311, 584, 23272, 281, 2316, 25993, 295, 264, 14375, 1412, 992, 13, 50842], "temperature": 0.0, "avg_logprob": -0.24533740234375, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.005908492486923933}, {"id": 329, "seek": 111520, "start": 1125.32, "end": 1128.8400000000001, "text": " It's already reached to the same perplexity in the pre-training stage.", "tokens": [50870, 467, 311, 1217, 6488, 281, 264, 912, 680, 18945, 507, 294, 264, 659, 12, 17227, 1760, 3233, 13, 51046], "temperature": 0.0, "avg_logprob": -0.24533740234375, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.005908492486923933}, {"id": 330, "seek": 111520, "start": 1129.2, "end": 1132.16, "text": " So, you know, that's the first, the rationale there.", "tokens": [51064, 407, 11, 291, 458, 11, 300, 311, 264, 700, 11, 264, 41989, 456, 13, 51212], "temperature": 0.0, "avg_logprob": -0.24533740234375, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.005908492486923933}, {"id": 331, "seek": 111520, "start": 1132.3600000000001, "end": 1136.4, "text": " And also like, since we don't have to load everything in the GPU, right?", "tokens": [51222, 400, 611, 411, 11, 1670, 321, 500, 380, 362, 281, 3677, 1203, 294, 264, 18407, 11, 558, 30, 51424], "temperature": 0.0, "avg_logprob": -0.24533740234375, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.005908492486923933}, {"id": 332, "seek": 111520, "start": 1136.4, "end": 1139.76, "text": " So there are kind of like a second again, there is basically the GPU memory.", "tokens": [51424, 407, 456, 366, 733, 295, 411, 257, 1150, 797, 11, 456, 307, 1936, 264, 18407, 4675, 13, 51592], "temperature": 0.0, "avg_logprob": -0.24533740234375, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.005908492486923933}, {"id": 333, "seek": 111520, "start": 1140.04, "end": 1142.76, "text": " And also most important thing is basically we compute things faster.", "tokens": [51606, 400, 611, 881, 1021, 551, 307, 1936, 321, 14722, 721, 4663, 13, 51742], "temperature": 0.0, "avg_logprob": -0.24533740234375, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.005908492486923933}, {"id": 334, "seek": 114276, "start": 1142.8, "end": 1147.56, "text": " So currently we can reach to 1.4 times like faster end to end training speed", "tokens": [50366, 407, 4362, 321, 393, 2524, 281, 502, 13, 19, 1413, 411, 4663, 917, 281, 917, 3097, 3073, 50604], "temperature": 0.0, "avg_logprob": -0.21951981151805205, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.02756914310157299}, {"id": 335, "seek": 114276, "start": 1147.72, "end": 1151.44, "text": " compared to the full rank or kind of like a dense llama model.", "tokens": [50612, 5347, 281, 264, 1577, 6181, 420, 733, 295, 411, 257, 18011, 23272, 2316, 13, 50798], "temperature": 0.0, "avg_logprob": -0.21951981151805205, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.02756914310157299}, {"id": 336, "seek": 114276, "start": 1151.76, "end": 1153.16, "text": " So those are the, the gain.", "tokens": [50814, 407, 729, 366, 264, 11, 264, 6052, 13, 50884], "temperature": 0.0, "avg_logprob": -0.21951981151805205, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.02756914310157299}, {"id": 337, "seek": 114276, "start": 1153.56, "end": 1157.6, "text": " The loose areas, like we kind of like a drop a little bit of the perplexity.", "tokens": [50904, 440, 9612, 3179, 11, 411, 321, 733, 295, 411, 257, 3270, 257, 707, 857, 295, 264, 680, 18945, 507, 13, 51106], "temperature": 0.0, "avg_logprob": -0.21951981151805205, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.02756914310157299}, {"id": 338, "seek": 114276, "start": 1157.64, "end": 1160.64, "text": " So roughly speaking the same, but the still a little bit of drop.", "tokens": [51108, 407, 9810, 4124, 264, 912, 11, 457, 264, 920, 257, 707, 857, 295, 3270, 13, 51258], "temperature": 0.0, "avg_logprob": -0.21951981151805205, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.02756914310157299}, {"id": 339, "seek": 114276, "start": 1160.64, "end": 1164.72, "text": " I'm still working on that tweaking different, I guess, hyperparameters", "tokens": [51258, 286, 478, 920, 1364, 322, 300, 6986, 2456, 819, 11, 286, 2041, 11, 9848, 2181, 335, 6202, 51462], "temperature": 0.0, "avg_logprob": -0.21951981151805205, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.02756914310157299}, {"id": 340, "seek": 114276, "start": 1164.8, "end": 1166.64, "text": " and then try to, try to make things better.", "tokens": [51466, 293, 550, 853, 281, 11, 853, 281, 652, 721, 1101, 13, 51558], "temperature": 0.0, "avg_logprob": -0.21951981151805205, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.02756914310157299}, {"id": 341, "seek": 114276, "start": 1167.0, "end": 1171.52, "text": " And in a lot of more general cases, I would have to do something with", "tokens": [51576, 400, 294, 257, 688, 295, 544, 2674, 3331, 11, 286, 576, 362, 281, 360, 746, 365, 51802], "temperature": 0.0, "avg_logprob": -0.21951981151805205, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.02756914310157299}, {"id": 342, "seek": 117152, "start": 1171.52, "end": 1174.92, "text": " my own hyperparameters, maybe a grid search or I don't know,", "tokens": [50364, 452, 1065, 9848, 2181, 335, 6202, 11, 1310, 257, 10748, 3164, 420, 286, 500, 380, 458, 11, 50534], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 343, "seek": 117152, "start": 1174.92, "end": 1176.4, "text": " some clever way of picking them.", "tokens": [50534, 512, 13494, 636, 295, 8867, 552, 13, 50608], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 344, "seek": 117152, "start": 1176.6399999999999, "end": 1179.96, "text": " Can I now step away from that and let Cuttlefish kind of auto", "tokens": [50620, 1664, 286, 586, 1823, 1314, 490, 300, 293, 718, 9431, 10972, 11608, 733, 295, 8399, 50786], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 345, "seek": 117152, "start": 1179.96, "end": 1181.84, "text": " optimize those sorts of things for me?", "tokens": [50786, 19719, 729, 7527, 295, 721, 337, 385, 30, 50880], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 346, "seek": 117152, "start": 1182.08, "end": 1187.48, "text": " So the best we can do is say, if we have a concrete set of hyperparameters", "tokens": [50892, 407, 264, 1151, 321, 393, 360, 307, 584, 11, 498, 321, 362, 257, 9859, 992, 295, 9848, 2181, 335, 6202, 51162], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 347, "seek": 117152, "start": 1187.48, "end": 1191.16, "text": " used for training the dense model, let's say llama, I think that's kind of like", "tokens": [51162, 1143, 337, 3097, 264, 18011, 2316, 11, 718, 311, 584, 23272, 11, 286, 519, 300, 311, 733, 295, 411, 51346], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 348, "seek": 117152, "start": 1191.16, "end": 1194.4, "text": " there's a standard like a set of hyperparameters, like what's the", "tokens": [51346, 456, 311, 257, 3832, 411, 257, 992, 295, 9848, 2181, 335, 6202, 11, 411, 437, 311, 264, 51508], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 349, "seek": 117152, "start": 1194.4, "end": 1195.76, "text": " learning rate that you need to use?", "tokens": [51508, 2539, 3314, 300, 291, 643, 281, 764, 30, 51576], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 350, "seek": 117152, "start": 1196.04, "end": 1197.24, "text": " What's that batch size?", "tokens": [51590, 708, 311, 300, 15245, 2744, 30, 51650], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 351, "seek": 117152, "start": 1197.24, "end": 1199.56, "text": " I think it's around two million things like that.", "tokens": [51650, 286, 519, 309, 311, 926, 732, 2459, 721, 411, 300, 13, 51766], "temperature": 0.0, "avg_logprob": -0.14931624094645182, "compression_ratio": 1.761744966442953, "no_speech_prob": 0.009408623911440372}, {"id": 352, "seek": 119956, "start": 1199.84, "end": 1203.76, "text": " So you have a kind of like a standard set of hyperparameter to get started with.", "tokens": [50378, 407, 291, 362, 257, 733, 295, 411, 257, 3832, 992, 295, 9848, 2181, 335, 2398, 281, 483, 1409, 365, 13, 50574], "temperature": 0.0, "avg_logprob": -0.21134278224064754, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.01064888946712017}, {"id": 353, "seek": 119956, "start": 1204.12, "end": 1208.08, "text": " Cuttlefish is doing something to guarantee you that if you still use the", "tokens": [50592, 9431, 10972, 11608, 307, 884, 746, 281, 10815, 291, 300, 498, 291, 920, 764, 264, 50790], "temperature": 0.0, "avg_logprob": -0.21134278224064754, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.01064888946712017}, {"id": 354, "seek": 119956, "start": 1208.08, "end": 1212.28, "text": " same set of hyperparameters, you will reach to a good accuracy.", "tokens": [50790, 912, 992, 295, 9848, 2181, 335, 6202, 11, 291, 486, 2524, 281, 257, 665, 14170, 13, 51000], "temperature": 0.0, "avg_logprob": -0.21134278224064754, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.01064888946712017}, {"id": 355, "seek": 119956, "start": 1212.6, "end": 1214.1599999999999, "text": " That's what we can guarantee.", "tokens": [51016, 663, 311, 437, 321, 393, 10815, 13, 51094], "temperature": 0.0, "avg_logprob": -0.21134278224064754, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.01064888946712017}, {"id": 356, "seek": 119956, "start": 1214.8, "end": 1218.6799999999998, "text": " But there's a more fundamental question that when you train low-rank model,", "tokens": [51126, 583, 456, 311, 257, 544, 8088, 1168, 300, 562, 291, 3847, 2295, 12, 20479, 2316, 11, 51320], "temperature": 0.0, "avg_logprob": -0.21134278224064754, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.01064888946712017}, {"id": 357, "seek": 119956, "start": 1219.28, "end": 1224.36, "text": " is there, you know, does there exist a better set of hyperparameter that can", "tokens": [51350, 307, 456, 11, 291, 458, 11, 775, 456, 2514, 257, 1101, 992, 295, 9848, 2181, 335, 2398, 300, 393, 51604], "temperature": 0.0, "avg_logprob": -0.21134278224064754, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.01064888946712017}, {"id": 358, "seek": 119956, "start": 1224.36, "end": 1228.48, "text": " make the high, you know, like a low-rank model to better the full-rank model?", "tokens": [51604, 652, 264, 1090, 11, 291, 458, 11, 411, 257, 2295, 12, 20479, 2316, 281, 1101, 264, 1577, 12, 20479, 2316, 30, 51810], "temperature": 0.0, "avg_logprob": -0.21134278224064754, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.01064888946712017}, {"id": 359, "seek": 122848, "start": 1228.76, "end": 1232.88, "text": " I think there is, but like from the design principle, we don't want to,", "tokens": [50378, 286, 519, 456, 307, 11, 457, 411, 490, 264, 1715, 8665, 11, 321, 500, 380, 528, 281, 11, 50584], "temperature": 0.0, "avg_logprob": -0.17032470387860763, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001366724376566708}, {"id": 360, "seek": 122848, "start": 1233.24, "end": 1235.56, "text": " you know, like dump that effort into users.", "tokens": [50602, 291, 458, 11, 411, 11430, 300, 4630, 666, 5022, 13, 50718], "temperature": 0.0, "avg_logprob": -0.17032470387860763, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001366724376566708}, {"id": 361, "seek": 122848, "start": 1235.96, "end": 1239.16, "text": " We just want to make the process as easy as possible.", "tokens": [50738, 492, 445, 528, 281, 652, 264, 1399, 382, 1858, 382, 1944, 13, 50898], "temperature": 0.0, "avg_logprob": -0.17032470387860763, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001366724376566708}, {"id": 362, "seek": 122848, "start": 1239.4, "end": 1243.28, "text": " You can just take the original set of hyperparameter, take our technology", "tokens": [50910, 509, 393, 445, 747, 264, 3380, 992, 295, 9848, 2181, 335, 2398, 11, 747, 527, 2899, 51104], "temperature": 0.0, "avg_logprob": -0.17032470387860763, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001366724376566708}, {"id": 363, "seek": 122848, "start": 1243.6, "end": 1245.6, "text": " and just train everything will be handled.", "tokens": [51120, 293, 445, 3847, 1203, 486, 312, 18033, 13, 51220], "temperature": 0.0, "avg_logprob": -0.17032470387860763, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001366724376566708}, {"id": 364, "seek": 122848, "start": 1245.64, "end": 1248.04, "text": " There's no extra hyperparameter to tune.", "tokens": [51222, 821, 311, 572, 2857, 9848, 2181, 335, 2398, 281, 10864, 13, 51342], "temperature": 0.0, "avg_logprob": -0.17032470387860763, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001366724376566708}, {"id": 365, "seek": 122848, "start": 1248.24, "end": 1252.04, "text": " Everything will converge almost as same as the original model.", "tokens": [51352, 5471, 486, 41881, 1920, 382, 912, 382, 264, 3380, 2316, 13, 51542], "temperature": 0.0, "avg_logprob": -0.17032470387860763, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001366724376566708}, {"id": 366, "seek": 122848, "start": 1252.44, "end": 1255.96, "text": " Well, I suppose everyone's data set is going to be a little bit different and", "tokens": [51562, 1042, 11, 286, 7297, 1518, 311, 1412, 992, 307, 516, 281, 312, 257, 707, 857, 819, 293, 51738], "temperature": 0.0, "avg_logprob": -0.17032470387860763, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.001366724376566708}, {"id": 367, "seek": 125596, "start": 1255.96, "end": 1261.24, "text": " unique, but at least in the paper you report it's 1.2 times faster to train", "tokens": [50364, 3845, 11, 457, 412, 1935, 294, 264, 3035, 291, 2275, 309, 311, 502, 13, 17, 1413, 4663, 281, 3847, 50628], "temperature": 0.0, "avg_logprob": -0.1650742676298497, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.06005137041211128}, {"id": 368, "seek": 125596, "start": 1261.24, "end": 1265.4, "text": " and 5.6 times smaller than a full-ranked model.", "tokens": [50628, 293, 1025, 13, 21, 1413, 4356, 813, 257, 1577, 12, 20479, 292, 2316, 13, 50836], "temperature": 0.0, "avg_logprob": -0.1650742676298497, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.06005137041211128}, {"id": 369, "seek": 125596, "start": 1265.48, "end": 1269.88, "text": " Is there any reason with those good stats in mind not to just adopt these processes?", "tokens": [50840, 1119, 456, 604, 1778, 365, 729, 665, 18152, 294, 1575, 406, 281, 445, 6878, 613, 7555, 30, 51060], "temperature": 0.0, "avg_logprob": -0.1650742676298497, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.06005137041211128}, {"id": 370, "seek": 125596, "start": 1270.56, "end": 1272.8, "text": " So I think that's probably my fault.", "tokens": [51094, 407, 286, 519, 300, 311, 1391, 452, 7441, 13, 51206], "temperature": 0.0, "avg_logprob": -0.1650742676298497, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.06005137041211128}, {"id": 371, "seek": 125596, "start": 1272.8, "end": 1278.16, "text": " Like, so we still need a very good software to basically support the things like that.", "tokens": [51206, 1743, 11, 370, 321, 920, 643, 257, 588, 665, 4722, 281, 1936, 1406, 264, 721, 411, 300, 13, 51474], "temperature": 0.0, "avg_logprob": -0.1650742676298497, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.06005137041211128}, {"id": 372, "seek": 125596, "start": 1278.16, "end": 1281.2, "text": " So, because there's a whole bunch of amazing algorithms.", "tokens": [51474, 407, 11, 570, 456, 311, 257, 1379, 3840, 295, 2243, 14642, 13, 51626], "temperature": 0.0, "avg_logprob": -0.1650742676298497, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.06005137041211128}, {"id": 373, "seek": 125596, "start": 1281.64, "end": 1285.3600000000001, "text": " I think one reason people are not using that is basically the software", "tokens": [51648, 286, 519, 472, 1778, 561, 366, 406, 1228, 300, 307, 1936, 264, 4722, 51834], "temperature": 0.0, "avg_logprob": -0.1650742676298497, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.06005137041211128}, {"id": 374, "seek": 128536, "start": 1285.52, "end": 1288.08, "text": " engineering has not been done right.", "tokens": [50372, 7043, 575, 406, 668, 1096, 558, 13, 50500], "temperature": 0.0, "avg_logprob": -0.23162892434449323, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0005791411967948079}, {"id": 375, "seek": 128536, "start": 1288.3999999999999, "end": 1293.76, "text": " So that's something I'm always trying to do is basically I want to write a model", "tokens": [50516, 407, 300, 311, 746, 286, 478, 1009, 1382, 281, 360, 307, 1936, 286, 528, 281, 2464, 257, 2316, 50784], "temperature": 0.0, "avg_logprob": -0.23162892434449323, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0005791411967948079}, {"id": 376, "seek": 128536, "start": 1293.76, "end": 1298.4799999999998, "text": " compiler, basically taking whatever kind of model and then, you know, the model", "tokens": [50784, 31958, 11, 1936, 1940, 2035, 733, 295, 2316, 293, 550, 11, 291, 458, 11, 264, 2316, 51020], "temperature": 0.0, "avg_logprob": -0.23162892434449323, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0005791411967948079}, {"id": 377, "seek": 128536, "start": 1298.4799999999998, "end": 1302.4799999999998, "text": " compiler will basically just go through all the layers and try to detect redundancy", "tokens": [51020, 31958, 486, 1936, 445, 352, 807, 439, 264, 7914, 293, 853, 281, 5531, 27830, 6717, 51220], "temperature": 0.0, "avg_logprob": -0.23162892434449323, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0005791411967948079}, {"id": 378, "seek": 128536, "start": 1302.4799999999998, "end": 1307.84, "text": " there. And then if the redundancy hit some of the criteria, it will basically just", "tokens": [51220, 456, 13, 400, 550, 498, 264, 27830, 6717, 2045, 512, 295, 264, 11101, 11, 309, 486, 1936, 445, 51488], "temperature": 0.0, "avg_logprob": -0.23162892434449323, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0005791411967948079}, {"id": 379, "seek": 128536, "start": 1307.84, "end": 1311.4799999999998, "text": " do the transfer that from full-rank to low-rank automatically.", "tokens": [51488, 360, 264, 5003, 300, 490, 1577, 12, 20479, 281, 2295, 12, 20479, 6772, 13, 51670], "temperature": 0.0, "avg_logprob": -0.23162892434449323, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0005791411967948079}, {"id": 380, "seek": 128536, "start": 1311.8, "end": 1313.84, "text": " I think that support is not there yet.", "tokens": [51686, 286, 519, 300, 1406, 307, 406, 456, 1939, 13, 51788], "temperature": 0.0, "avg_logprob": -0.23162892434449323, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0005791411967948079}, {"id": 381, "seek": 131384, "start": 1313.84, "end": 1316.12, "text": " So that's I should work on that.", "tokens": [50364, 407, 300, 311, 286, 820, 589, 322, 300, 13, 50478], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 382, "seek": 131384, "start": 1316.12, "end": 1318.9599999999998, "text": " And I'm actually trying to work on that right now.", "tokens": [50478, 400, 286, 478, 767, 1382, 281, 589, 322, 300, 558, 586, 13, 50620], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 383, "seek": 131384, "start": 1319.6, "end": 1320.56, "text": " So that's the first thing.", "tokens": [50652, 407, 300, 311, 264, 700, 551, 13, 50700], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 384, "seek": 131384, "start": 1321.08, "end": 1325.32, "text": " The second thing is basically, as you mentioned, for different applications, I think", "tokens": [50726, 440, 1150, 551, 307, 1936, 11, 382, 291, 2835, 11, 337, 819, 5821, 11, 286, 519, 50938], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 385, "seek": 131384, "start": 1325.52, "end": 1329.76, "text": " there are still some of the hyperparameter has to be tuned appropriately.", "tokens": [50948, 456, 366, 920, 512, 295, 264, 9848, 2181, 335, 2398, 575, 281, 312, 10870, 23505, 13, 51160], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 386, "seek": 131384, "start": 1330.0, "end": 1334.6399999999999, "text": " Otherwise, I'm not sure if the low-rank method will work as good as, let's say,", "tokens": [51172, 10328, 11, 286, 478, 406, 988, 498, 264, 2295, 12, 20479, 3170, 486, 589, 382, 665, 382, 11, 718, 311, 584, 11, 51404], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 387, "seek": 131384, "start": 1334.6399999999999, "end": 1336.24, "text": " the vision and the language models.", "tokens": [51404, 264, 5201, 293, 264, 2856, 5245, 13, 51484], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 388, "seek": 131384, "start": 1336.28, "end": 1338.28, "text": " Those are the two mainstream models.", "tokens": [51486, 3950, 366, 264, 732, 15960, 5245, 13, 51586], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 389, "seek": 131384, "start": 1338.32, "end": 1342.9599999999998, "text": " But I think a lot of people care about, let's say, graph data and or even like", "tokens": [51588, 583, 286, 519, 257, 688, 295, 561, 1127, 466, 11, 718, 311, 584, 11, 4295, 1412, 293, 420, 754, 411, 51820], "temperature": 0.0, "avg_logprob": -0.19643905583549948, "compression_ratio": 1.6983050847457628, "no_speech_prob": 0.00029128286405466497}, {"id": 390, "seek": 134296, "start": 1343.0, "end": 1347.52, "text": " tabular data. So those are things like I haven't explored very well yet.", "tokens": [50366, 4421, 1040, 1412, 13, 407, 729, 366, 721, 411, 286, 2378, 380, 24016, 588, 731, 1939, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1578792365821632, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0005439892993308604}, {"id": 391, "seek": 134296, "start": 1347.64, "end": 1351.4, "text": " Yeah. But I think the most most important part is the software engineering that has", "tokens": [50598, 865, 13, 583, 286, 519, 264, 881, 881, 1021, 644, 307, 264, 4722, 7043, 300, 575, 50786], "temperature": 0.0, "avg_logprob": -0.1578792365821632, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0005439892993308604}, {"id": 392, "seek": 134296, "start": 1351.4, "end": 1356.4, "text": " been done appropriately and very efficiently, such that people would like to", "tokens": [50786, 668, 1096, 23505, 293, 588, 19621, 11, 1270, 300, 561, 576, 411, 281, 51036], "temperature": 0.0, "avg_logprob": -0.1578792365821632, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0005439892993308604}, {"id": 393, "seek": 134296, "start": 1356.4, "end": 1357.04, "text": " really adopt.", "tokens": [51036, 534, 6878, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1578792365821632, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0005439892993308604}, {"id": 394, "seek": 134296, "start": 1357.8, "end": 1361.04, "text": " Well, when I think historically, if someone said, I'm interested in machine", "tokens": [51106, 1042, 11, 562, 286, 519, 16180, 11, 498, 1580, 848, 11, 286, 478, 3102, 294, 3479, 51268], "temperature": 0.0, "avg_logprob": -0.1578792365821632, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0005439892993308604}, {"id": 395, "seek": 134296, "start": 1361.04, "end": 1365.32, "text": " learning, I would encourage them to get, you know, probably an advanced degree,", "tokens": [51268, 2539, 11, 286, 576, 5373, 552, 281, 483, 11, 291, 458, 11, 1391, 364, 7339, 4314, 11, 51482], "temperature": 0.0, "avg_logprob": -0.1578792365821632, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0005439892993308604}, {"id": 396, "seek": 134296, "start": 1365.32, "end": 1369.16, "text": " focus in math and stats and algorithms and things along these lines.", "tokens": [51482, 1879, 294, 5221, 293, 18152, 293, 14642, 293, 721, 2051, 613, 3876, 13, 51674], "temperature": 0.0, "avg_logprob": -0.1578792365821632, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.0005439892993308604}, {"id": 397, "seek": 136916, "start": 1369.68, "end": 1374.0400000000002, "text": " It seems like maybe there are more lines of opportunity in machine learning now.", "tokens": [50390, 467, 2544, 411, 1310, 456, 366, 544, 3876, 295, 2650, 294, 3479, 2539, 586, 13, 50608], "temperature": 0.0, "avg_logprob": -0.15633618949663522, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.014492696151137352}, {"id": 398, "seek": 136916, "start": 1374.0800000000002, "end": 1378.76, "text": " For example, like people interested in software are not always interested in", "tokens": [50610, 1171, 1365, 11, 411, 561, 3102, 294, 4722, 366, 406, 1009, 3102, 294, 50844], "temperature": 0.0, "avg_logprob": -0.15633618949663522, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.014492696151137352}, {"id": 399, "seek": 136916, "start": 1378.76, "end": 1381.16, "text": " compilers, but some people are compiler people.", "tokens": [50844, 715, 388, 433, 11, 457, 512, 561, 366, 31958, 561, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15633618949663522, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.014492696151137352}, {"id": 400, "seek": 136916, "start": 1381.3200000000002, "end": 1385.8000000000002, "text": " Do you have any advice for people who want to take a sort of really engineering", "tokens": [50972, 1144, 291, 362, 604, 5192, 337, 561, 567, 528, 281, 747, 257, 1333, 295, 534, 7043, 51196], "temperature": 0.0, "avg_logprob": -0.15633618949663522, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.014492696151137352}, {"id": 401, "seek": 136916, "start": 1385.8000000000002, "end": 1387.28, "text": " MLOps kind of approach?", "tokens": [51196, 376, 20184, 1878, 733, 295, 3109, 30, 51270], "temperature": 0.0, "avg_logprob": -0.15633618949663522, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.014492696151137352}, {"id": 402, "seek": 136916, "start": 1387.3200000000002, "end": 1389.68, "text": " What are good inroads to be learning this area?", "tokens": [51272, 708, 366, 665, 294, 30342, 281, 312, 2539, 341, 1859, 30, 51390], "temperature": 0.0, "avg_logprob": -0.15633618949663522, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.014492696151137352}, {"id": 403, "seek": 136916, "start": 1391.0, "end": 1393.96, "text": " I would really recommend people to just start to play with.", "tokens": [51456, 286, 576, 534, 2748, 561, 281, 445, 722, 281, 862, 365, 13, 51604], "temperature": 0.0, "avg_logprob": -0.15633618949663522, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.014492696151137352}, {"id": 404, "seek": 139396, "start": 1394.04, "end": 1400.68, "text": " Let's say just download the Lama2, you know, just the checkpoint and just start", "tokens": [50368, 961, 311, 584, 445, 5484, 264, 441, 2404, 17, 11, 291, 458, 11, 445, 264, 42269, 293, 445, 722, 50700], "temperature": 0.0, "avg_logprob": -0.3397890853881836, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.045975785702466965}, {"id": 405, "seek": 139396, "start": 1400.68, "end": 1404.96, "text": " to, let's say, put that into or just fine tune on their own data.", "tokens": [50700, 281, 11, 718, 311, 584, 11, 829, 300, 666, 420, 445, 2489, 10864, 322, 641, 1065, 1412, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3397890853881836, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.045975785702466965}, {"id": 406, "seek": 139396, "start": 1405.2, "end": 1406.64, "text": " I just use a hugging face.", "tokens": [50926, 286, 445, 764, 257, 41706, 1851, 13, 50998], "temperature": 0.0, "avg_logprob": -0.3397890853881836, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.045975785702466965}, {"id": 407, "seek": 139396, "start": 1406.8400000000001, "end": 1411.6000000000001, "text": " I think hugging face support is kind of like a provide very good software support", "tokens": [51008, 286, 519, 41706, 1851, 1406, 307, 733, 295, 411, 257, 2893, 588, 665, 4722, 1406, 51246], "temperature": 0.0, "avg_logprob": -0.3397890853881836, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.045975785702466965}, {"id": 408, "seek": 139396, "start": 1411.8400000000001, "end": 1416.88, "text": " for everything. And just start to try to play and process the data, the tokenized", "tokens": [51258, 337, 1203, 13, 400, 445, 722, 281, 853, 281, 862, 293, 1399, 264, 1412, 11, 264, 14862, 1602, 51510], "temperature": 0.0, "avg_logprob": -0.3397890853881836, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.045975785702466965}, {"id": 409, "seek": 139396, "start": 1416.88, "end": 1418.76, "text": " things, like what does that even mean?", "tokens": [51510, 721, 11, 411, 437, 775, 300, 754, 914, 30, 51604], "temperature": 0.0, "avg_logprob": -0.3397890853881836, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.045975785702466965}, {"id": 410, "seek": 141876, "start": 1419.08, "end": 1423.76, "text": " And just the factor, I think, you know, just start to do the fine tuning.", "tokens": [50380, 400, 445, 264, 5952, 11, 286, 519, 11, 291, 458, 11, 445, 722, 281, 360, 264, 2489, 15164, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 411, "seek": 141876, "start": 1424.12, "end": 1426.6, "text": " And then when they are working on that, then we will start to see, you know,", "tokens": [50632, 400, 550, 562, 436, 366, 1364, 322, 300, 11, 550, 321, 486, 722, 281, 536, 11, 291, 458, 11, 50756], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 412, "seek": 141876, "start": 1426.6, "end": 1428.2, "text": " what's actually the task is, right?", "tokens": [50756, 437, 311, 767, 264, 5633, 307, 11, 558, 30, 50836], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 413, "seek": 141876, "start": 1428.2, "end": 1430.04, "text": " So it's just a basic next-door prediction.", "tokens": [50836, 407, 309, 311, 445, 257, 3875, 958, 12, 10441, 17630, 13, 50928], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 414, "seek": 141876, "start": 1430.44, "end": 1434.2, "text": " And then they will kind of like try to realize which part doesn't go right.", "tokens": [50948, 400, 550, 436, 486, 733, 295, 411, 853, 281, 4325, 597, 644, 1177, 380, 352, 558, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 415, "seek": 141876, "start": 1434.6, "end": 1438.48, "text": " And what does the loss function mean?", "tokens": [51156, 400, 437, 775, 264, 4470, 2445, 914, 30, 51350], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 416, "seek": 141876, "start": 1438.48, "end": 1439.72, "text": " What does gradient mean?", "tokens": [51350, 708, 775, 16235, 914, 30, 51412], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 417, "seek": 141876, "start": 1439.84, "end": 1442.08, "text": " Yeah, I think everything will naturally come into play.", "tokens": [51418, 865, 11, 286, 519, 1203, 486, 8195, 808, 666, 862, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 418, "seek": 141876, "start": 1442.44, "end": 1443.44, "text": " Good advice. Yeah.", "tokens": [51548, 2205, 5192, 13, 865, 13, 51598], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 419, "seek": 141876, "start": 1443.48, "end": 1445.8, "text": " I learned a lot by getting my hands dirty.", "tokens": [51600, 286, 3264, 257, 688, 538, 1242, 452, 2377, 9360, 13, 51716], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 420, "seek": 141876, "start": 1445.8, "end": 1448.0, "text": " I didn't learn everything in a classroom, for sure.", "tokens": [51716, 286, 994, 380, 1466, 1203, 294, 257, 7419, 11, 337, 988, 13, 51826], "temperature": 0.0, "avg_logprob": -0.2514367583614068, "compression_ratio": 1.7467532467532467, "no_speech_prob": 0.03731045499444008}, {"id": 421, "seek": 144876, "start": 1448.92, "end": 1452.0, "text": " Same thing here. Well, yeah, classroom is also good.", "tokens": [50372, 10635, 551, 510, 13, 1042, 11, 1338, 11, 7419, 307, 611, 665, 13, 50526], "temperature": 0.0, "avg_logprob": -0.285533297378405, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0005031495820730925}, {"id": 422, "seek": 144876, "start": 1452.0, "end": 1457.8799999999999, "text": " And yeah, I guess people still, if you can go to college or a graduate school,", "tokens": [50526, 400, 1338, 11, 286, 2041, 561, 920, 11, 498, 291, 393, 352, 281, 3859, 420, 257, 8080, 1395, 11, 50820], "temperature": 0.0, "avg_logprob": -0.285533297378405, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0005031495820730925}, {"id": 423, "seek": 144876, "start": 1457.8799999999999, "end": 1461.36, "text": " I think the principle learning, machine learning in the principle of the way is", "tokens": [50820, 286, 519, 264, 8665, 2539, 11, 3479, 2539, 294, 264, 8665, 295, 264, 636, 307, 50994], "temperature": 0.0, "avg_logprob": -0.285533297378405, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0005031495820730925}, {"id": 424, "seek": 144876, "start": 1461.36, "end": 1465.32, "text": " still important to understand, you know, what's actually going on, you know, what", "tokens": [50994, 920, 1021, 281, 1223, 11, 291, 458, 11, 437, 311, 767, 516, 322, 11, 291, 458, 11, 437, 51192], "temperature": 0.0, "avg_logprob": -0.285533297378405, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0005031495820730925}, {"id": 425, "seek": 144876, "start": 1465.32, "end": 1467.6, "text": " does generalization mean, things like that.", "tokens": [51192, 775, 2674, 2144, 914, 11, 721, 411, 300, 13, 51306], "temperature": 0.0, "avg_logprob": -0.285533297378405, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0005031495820730925}, {"id": 426, "seek": 144876, "start": 1467.76, "end": 1472.36, "text": " But currently, it seems like the whole field is going into the, I think, toward", "tokens": [51314, 583, 4362, 11, 309, 2544, 411, 264, 1379, 2519, 307, 516, 666, 264, 11, 286, 519, 11, 7361, 51544], "temperature": 0.0, "avg_logprob": -0.285533297378405, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0005031495820730925}, {"id": 427, "seek": 144876, "start": 1472.36, "end": 1476.0, "text": " application engineering heavy style.", "tokens": [51544, 3861, 7043, 4676, 3758, 13, 51726], "temperature": 0.0, "avg_logprob": -0.285533297378405, "compression_ratio": 1.6940298507462686, "no_speech_prob": 0.0005031495820730925}, {"id": 428, "seek": 147600, "start": 1476.24, "end": 1480.52, "text": " So that's why I think practice would be really be important.", "tokens": [50376, 407, 300, 311, 983, 286, 519, 3124, 576, 312, 534, 312, 1021, 13, 50590], "temperature": 0.0, "avg_logprob": -0.23029074175604458, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.019107161089777946}, {"id": 429, "seek": 147600, "start": 1480.84, "end": 1484.44, "text": " And people, as you mentioned, I really agree with you, Kyle, like people should", "tokens": [50606, 400, 561, 11, 382, 291, 2835, 11, 286, 534, 3986, 365, 291, 11, 18023, 11, 411, 561, 820, 50786], "temperature": 0.0, "avg_logprob": -0.23029074175604458, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.019107161089777946}, {"id": 430, "seek": 147600, "start": 1484.44, "end": 1487.88, "text": " get their hands dirty and try to really experience things.", "tokens": [50786, 483, 641, 2377, 9360, 293, 853, 281, 534, 1752, 721, 13, 50958], "temperature": 0.0, "avg_logprob": -0.23029074175604458, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.019107161089777946}, {"id": 431, "seek": 147600, "start": 1488.24, "end": 1490.32, "text": " What's coming next for you in your research?", "tokens": [50976, 708, 311, 1348, 958, 337, 291, 294, 428, 2132, 30, 51080], "temperature": 0.0, "avg_logprob": -0.23029074175604458, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.019107161089777946}, {"id": 432, "seek": 147600, "start": 1490.72, "end": 1495.6, "text": " My vision is mostly currently like to into kind of two separated fields.", "tokens": [51100, 1222, 5201, 307, 5240, 4362, 411, 281, 666, 733, 295, 732, 12005, 7909, 13, 51344], "temperature": 0.0, "avg_logprob": -0.23029074175604458, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.019107161089777946}, {"id": 433, "seek": 147600, "start": 1495.8, "end": 1500.4, "text": " As for people in academia, we shouldn't really compete with, try to compete with", "tokens": [51354, 1018, 337, 561, 294, 28937, 11, 321, 4659, 380, 534, 11831, 365, 11, 853, 281, 11831, 365, 51584], "temperature": 0.0, "avg_logprob": -0.23029074175604458, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.019107161089777946}, {"id": 434, "seek": 147600, "start": 1500.4, "end": 1504.52, "text": " OpenAI. I think that's a little bit impossible to achieve AGI there.", "tokens": [51584, 7238, 48698, 13, 286, 519, 300, 311, 257, 707, 857, 6243, 281, 4584, 316, 26252, 456, 13, 51790], "temperature": 0.0, "avg_logprob": -0.23029074175604458, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.019107161089777946}, {"id": 435, "seek": 150452, "start": 1504.76, "end": 1510.0, "text": " What we should really do in the MLCs community is basically I'm interested in", "tokens": [50376, 708, 321, 820, 534, 360, 294, 264, 376, 14766, 82, 1768, 307, 1936, 286, 478, 3102, 294, 50638], "temperature": 0.0, "avg_logprob": -0.21359288113788494, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0019258828833699226}, {"id": 436, "seek": 150452, "start": 1510.0, "end": 1513.0, "text": " helping, let's say, people do fundamental science.", "tokens": [50638, 4315, 11, 718, 311, 584, 11, 561, 360, 8088, 3497, 13, 50788], "temperature": 0.0, "avg_logprob": -0.21359288113788494, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0019258828833699226}, {"id": 437, "seek": 150452, "start": 1513.24, "end": 1516.08, "text": " There's a whole area called AI for Science.", "tokens": [50800, 821, 311, 257, 1379, 1859, 1219, 7318, 337, 8976, 13, 50942], "temperature": 0.0, "avg_logprob": -0.21359288113788494, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0019258828833699226}, {"id": 438, "seek": 150452, "start": 1516.2, "end": 1520.92, "text": " I see big opportunities there that people can leverage the power of foundation", "tokens": [50948, 286, 536, 955, 4786, 456, 300, 561, 393, 13982, 264, 1347, 295, 7030, 51184], "temperature": 0.0, "avg_logprob": -0.21359288113788494, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0019258828833699226}, {"id": 439, "seek": 150452, "start": 1520.92, "end": 1522.76, "text": " model to really help their applications.", "tokens": [51184, 2316, 281, 534, 854, 641, 5821, 13, 51276], "temperature": 0.0, "avg_logprob": -0.21359288113788494, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0019258828833699226}, {"id": 440, "seek": 150452, "start": 1522.76, "end": 1528.32, "text": " Let's say gene protein structure prediction, like a drug prediction, and also", "tokens": [51276, 961, 311, 584, 12186, 7944, 3877, 17630, 11, 411, 257, 4110, 17630, 11, 293, 611, 51554], "temperature": 0.0, "avg_logprob": -0.21359288113788494, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0019258828833699226}, {"id": 441, "seek": 150452, "start": 1528.32, "end": 1532.84, "text": " the kind of like a medical diagnosis assistant I just mentioned to you.", "tokens": [51554, 264, 733, 295, 411, 257, 4625, 15217, 10994, 286, 445, 2835, 281, 291, 13, 51780], "temperature": 0.0, "avg_logprob": -0.21359288113788494, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.0019258828833699226}, {"id": 442, "seek": 153284, "start": 1533.72, "end": 1535.36, "text": " So that's the one thing I'm trying to do right now.", "tokens": [50408, 407, 300, 311, 264, 472, 551, 286, 478, 1382, 281, 360, 558, 586, 13, 50490], "temperature": 0.0, "avg_logprob": -0.19126683657931298, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0003405280294828117}, {"id": 443, "seek": 153284, "start": 1535.36, "end": 1540.52, "text": " So how can we leverage the powerfulness of the foundation model to build", "tokens": [50490, 407, 577, 393, 321, 13982, 264, 1347, 26872, 295, 264, 7030, 2316, 281, 1322, 50748], "temperature": 0.0, "avg_logprob": -0.19126683657931298, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0003405280294828117}, {"id": 444, "seek": 153284, "start": 1540.52, "end": 1546.4399999999998, "text": " the domain specific applications and really to help like mostly scientists in", "tokens": [50748, 264, 9274, 2685, 5821, 293, 534, 281, 854, 411, 5240, 7708, 294, 51044], "temperature": 0.0, "avg_logprob": -0.19126683657931298, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0003405280294828117}, {"id": 445, "seek": 153284, "start": 1546.4399999999998, "end": 1551.4399999999998, "text": " my case? And another is basically how to democratize, really democratize the", "tokens": [51044, 452, 1389, 30, 400, 1071, 307, 1936, 577, 281, 37221, 1125, 11, 534, 37221, 1125, 264, 51294], "temperature": 0.0, "avg_logprob": -0.19126683657931298, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0003405280294828117}, {"id": 446, "seek": 153284, "start": 1552.04, "end": 1555.4399999999998, "text": " foundation model to wider, you know, amount of people.", "tokens": [51324, 7030, 2316, 281, 11842, 11, 291, 458, 11, 2372, 295, 561, 13, 51494], "temperature": 0.0, "avg_logprob": -0.19126683657931298, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0003405280294828117}, {"id": 447, "seek": 153284, "start": 1555.4399999999998, "end": 1560.72, "text": " So like when people do not have like advanced computing resources, what can we", "tokens": [51494, 407, 411, 562, 561, 360, 406, 362, 411, 7339, 15866, 3593, 11, 437, 393, 321, 51758], "temperature": 0.0, "avg_logprob": -0.19126683657931298, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.0003405280294828117}, {"id": 448, "seek": 156072, "start": 1560.72, "end": 1565.8, "text": " do for them? So the software can be optimized really well to make people also", "tokens": [50364, 360, 337, 552, 30, 407, 264, 4722, 393, 312, 26941, 534, 731, 281, 652, 561, 611, 50618], "temperature": 0.0, "avg_logprob": -0.17263646806989397, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.006287015043199062}, {"id": 449, "seek": 156072, "start": 1565.8, "end": 1567.6000000000001, "text": " use foundation model really well.", "tokens": [50618, 764, 7030, 2316, 534, 731, 13, 50708], "temperature": 0.0, "avg_logprob": -0.17263646806989397, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.006287015043199062}, {"id": 450, "seek": 156072, "start": 1567.72, "end": 1572.8, "text": " It turns out actually we can also like group the commodity hardware together to", "tokens": [50714, 467, 4523, 484, 767, 321, 393, 611, 411, 1594, 264, 29125, 8837, 1214, 281, 50968], "temperature": 0.0, "avg_logprob": -0.17263646806989397, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.006287015043199062}, {"id": 451, "seek": 156072, "start": 1572.8, "end": 1575.08, "text": " even train the powerful foundation model.", "tokens": [50968, 754, 3847, 264, 4005, 7030, 2316, 13, 51082], "temperature": 0.0, "avg_logprob": -0.17263646806989397, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.006287015043199062}, {"id": 452, "seek": 156072, "start": 1575.28, "end": 1581.6000000000001, "text": " So that's what I'm excited about for the future research.", "tokens": [51092, 407, 300, 311, 437, 286, 478, 2919, 466, 337, 264, 2027, 2132, 13, 51408], "temperature": 0.0, "avg_logprob": -0.17263646806989397, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.006287015043199062}, {"id": 453, "seek": 156072, "start": 1581.76, "end": 1584.92, "text": " And is there anywhere listeners can follow you online if they want to keep up", "tokens": [51416, 400, 307, 456, 4992, 23274, 393, 1524, 291, 2950, 498, 436, 528, 281, 1066, 493, 51574], "temperature": 0.0, "avg_logprob": -0.17263646806989397, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.006287015043199062}, {"id": 454, "seek": 156072, "start": 1584.92, "end": 1585.6000000000001, "text": " with that as well?", "tokens": [51574, 365, 300, 382, 731, 30, 51608], "temperature": 0.0, "avg_logprob": -0.17263646806989397, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.006287015043199062}, {"id": 455, "seek": 156072, "start": 1586.04, "end": 1589.1200000000001, "text": " Yeah, definitely. So one thing is definitely Twitter.", "tokens": [51630, 865, 11, 2138, 13, 407, 472, 551, 307, 2138, 5794, 13, 51784], "temperature": 0.0, "avg_logprob": -0.17263646806989397, "compression_ratio": 1.6679245283018869, "no_speech_prob": 0.006287015043199062}, {"id": 456, "seek": 158912, "start": 1589.12, "end": 1593.52, "text": " I think I already shared Twitter, my Twitter account.", "tokens": [50364, 286, 519, 286, 1217, 5507, 5794, 11, 452, 5794, 2696, 13, 50584], "temperature": 0.0, "avg_logprob": -0.21025560033602977, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0026290647219866514}, {"id": 457, "seek": 158912, "start": 1593.7199999999998, "end": 1598.6, "text": " And I think another is basically my Google Scholar page and my personal home page.", "tokens": [50594, 400, 286, 519, 1071, 307, 1936, 452, 3329, 2065, 15276, 3028, 293, 452, 2973, 1280, 3028, 13, 50838], "temperature": 0.0, "avg_logprob": -0.21025560033602977, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0026290647219866514}, {"id": 458, "seek": 158912, "start": 1598.76, "end": 1603.1999999999998, "text": " Yeah, I can probably send it to you after this, after the recording.", "tokens": [50846, 865, 11, 286, 393, 1391, 2845, 309, 281, 291, 934, 341, 11, 934, 264, 6613, 13, 51068], "temperature": 0.0, "avg_logprob": -0.21025560033602977, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0026290647219866514}, {"id": 459, "seek": 158912, "start": 1603.76, "end": 1604.6399999999999, "text": " That'd be good. Yeah.", "tokens": [51096, 663, 1116, 312, 665, 13, 865, 13, 51140], "temperature": 0.0, "avg_logprob": -0.21025560033602977, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0026290647219866514}, {"id": 460, "seek": 158912, "start": 1604.6399999999999, "end": 1607.84, "text": " We'll have links to all the above in the show notes for people to follow up with.", "tokens": [51140, 492, 603, 362, 6123, 281, 439, 264, 3673, 294, 264, 855, 5570, 337, 561, 281, 1524, 493, 365, 13, 51300], "temperature": 0.0, "avg_logprob": -0.21025560033602977, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0026290647219866514}, {"id": 461, "seek": 158912, "start": 1608.0, "end": 1611.0, "text": " Well, thank you so much for taking the time to come on and share your work.", "tokens": [51308, 1042, 11, 1309, 291, 370, 709, 337, 1940, 264, 565, 281, 808, 322, 293, 2073, 428, 589, 13, 51458], "temperature": 0.0, "avg_logprob": -0.21025560033602977, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0026290647219866514}, {"id": 462, "seek": 158912, "start": 1611.12, "end": 1613.08, "text": " Oh, yeah. Thanks a lot for having me, Kyle.", "tokens": [51464, 876, 11, 1338, 13, 2561, 257, 688, 337, 1419, 385, 11, 18023, 13, 51562], "temperature": 0.0, "avg_logprob": -0.21025560033602977, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0026290647219866514}, {"id": 463, "seek": 158912, "start": 1613.08, "end": 1615.08, "text": " That's this is really, really exciting.", "tokens": [51562, 663, 311, 341, 307, 534, 11, 534, 4670, 13, 51662], "temperature": 0.0, "avg_logprob": -0.21025560033602977, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.0026290647219866514}], "language": "en"}